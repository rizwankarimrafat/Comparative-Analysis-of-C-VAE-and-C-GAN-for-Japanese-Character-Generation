{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5a31db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "98bec26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training datasets\n",
    "train_imgs_path = 'k49-train-imgs.npz'\n",
    "train_labels_path = 'k49-train-labels.npz'\n",
    "\n",
    "train_imgs_data = np.load(train_imgs_path)\n",
    "train_labels_data = np.load(train_labels_path)\n",
    "\n",
    "train_images = train_imgs_data['arr_0']\n",
    "train_labels = train_labels_data['arr_0']\n",
    "\n",
    "# Load the testing datasets\n",
    "test_imgs_path = 'k49-test-imgs.npz'\n",
    "test_labels_path = 'k49-test-labels.npz'\n",
    "\n",
    "test_imgs_data = np.load(test_imgs_path)\n",
    "test_labels_data = np.load(test_labels_path)\n",
    "\n",
    "test_images = test_imgs_data['arr_0']\n",
    "test_labels = test_labels_data['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9013e8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold value for distinguishing foreground\n",
    "threshold = 0\n",
    "\n",
    "# Calculate number of foreground pixels for each image in the training set\n",
    "train_foreground_pixel_counts = np.sum(train_images > threshold, axis=(1, 2))\n",
    "\n",
    "# Calculate number of foreground pixels for each image in the testing set\n",
    "test_foreground_pixel_counts = np.sum(test_images > threshold, axis=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a2f5d1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_pixel_counts = {i: [] for i in range(49)}\n",
    "\n",
    "for label, pixel_count in zip(train_labels, train_foreground_pixel_counts):\n",
    "    class_pixel_counts[label].append(pixel_count)\n",
    "\n",
    "# Calculate median for each class\n",
    "class_medians = {cls: np.median(counts) for cls, counts in class_pixel_counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f9d0785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_style_labels = []\n",
    "\n",
    "for label, pixel_count in zip(train_labels, train_foreground_pixel_counts):\n",
    "    if pixel_count > class_medians[label]:\n",
    "        train_style_labels.append(1)  # 'thick'\n",
    "    else:\n",
    "        train_style_labels.append(0)  # 'thin'\n",
    "\n",
    "train_style_labels = np.array(train_style_labels)\n",
    "\n",
    "test_style_labels = []\n",
    "\n",
    "for label, pixel_count in zip(test_labels, test_foreground_pixel_counts):\n",
    "    if pixel_count > class_medians[label]:\n",
    "        test_style_labels.append(1)  # 'thick'\n",
    "    else:\n",
    "        test_style_labels.append(0)  # 'thin'\n",
    "\n",
    "test_style_labels = np.array(test_style_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d12452d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training labels with style\n",
    "np.savez('k49-train-labels-with-style.npz', labels=train_labels, style_labels=train_style_labels)\n",
    "\n",
    "# Save testing labels with style\n",
    "np.savez('k49-test-labels-with-style.npz', labels=test_labels, style_labels=test_style_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "abf21d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs_path = 'k49-train-imgs.npz'\n",
    "train_labels_path = 'k49-train-labels-with-style.npz'\n",
    "\n",
    "train_images = np.load(train_imgs_path)['arr_0']\n",
    "train_data = np.load(train_labels_path)\n",
    "train_labels = train_data['labels']\n",
    "train_style_labels = train_data['style_labels']\n",
    "\n",
    "test_imgs_path = 'k49-test-imgs.npz'\n",
    "test_labels_path = 'k49-test-labels-with-style.npz'\n",
    "\n",
    "test_images = np.load(test_imgs_path)['arr_0']\n",
    "test_data = np.load(test_labels_path)\n",
    "test_labels = test_data['labels']\n",
    "test_style_labels = test_data['style_labels']\n",
    "\n",
    "# Normalize images\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Flatten images and convert to PyTorch tensors\n",
    "train_images = torch.tensor(train_images, dtype=torch.float32).view(-1, 784)\n",
    "test_images = torch.tensor(test_images, dtype=torch.float32).view(-1, 784)\n",
    "\n",
    "# One-hot encode labels and styles\n",
    "num_classes = 49\n",
    "num_styles = 2\n",
    "\n",
    "train_labels = train_labels.astype(int)\n",
    "train_style_labels = train_style_labels.astype(int)\n",
    "\n",
    "train_labels_one_hot = torch.eye(num_classes)[train_labels]\n",
    "train_style_labels_one_hot = torch.eye(num_styles)[train_style_labels]\n",
    "\n",
    "test_labels = test_labels.astype(int)\n",
    "test_style_labels = test_style_labels.astype(int)\n",
    "\n",
    "test_labels_one_hot = torch.eye(num_classes)[test_labels]\n",
    "test_style_labels_one_hot = torch.eye(num_styles)[test_style_labels]\n",
    "\n",
    "# Create DataLoader for training and testing\n",
    "train_dataset = TensorDataset(train_images, train_labels_one_hot, train_style_labels_one_hot)\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_images, test_labels_one_hot, test_style_labels_one_hot)\n",
    "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5147f8c6",
   "metadata": {},
   "source": [
    "# => Conditional Variational Autoencoder (C-VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "21f2707f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, num_classes, num_styles):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.num_styles = num_styles\n",
    "\n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_dim + num_classes + num_styles, hidden_dim)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.fc3 = nn.Linear(latent_dim + num_classes + num_styles, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def encode(self, x, c, s):\n",
    "        x = torch.cat((x, c, s), dim=1)\n",
    "        h = F.relu(self.fc1(x))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, c, s):\n",
    "        z = torch.cat((z, c, s), dim=1)\n",
    "        h = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h))\n",
    "\n",
    "    def forward(self, x, c, s):\n",
    "        mu, logvar = self.encode(x.view(-1, self.input_dim), c, s)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z, c, s), mu, logvar\n",
    "    \n",
    "def cvae_loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "2f987d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "input_size = 784\n",
    "latent_dim = 2\n",
    "hidden_sqrt = 12\n",
    "hidden_size = hidden_sqrt * hidden_sqrt\n",
    "num_epochs = 40\n",
    "learning_rate = 0.001\n",
    "\n",
    "cvae = CVAE(input_size, hidden_size, latent_dim, num_classes, num_styles)\n",
    "optimizer = optim.Adam(cvae.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "236d3317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 268.4778\n",
      "Epoch 2, Loss: 248.3789\n",
      "Epoch 3, Loss: 243.5916\n",
      "Epoch 4, Loss: 240.6472\n",
      "Epoch 5, Loss: 238.6640\n",
      "Epoch 6, Loss: 237.2555\n",
      "Epoch 7, Loss: 236.1907\n",
      "Epoch 8, Loss: 235.3184\n",
      "Epoch 9, Loss: 234.6131\n",
      "Epoch 10, Loss: 233.9962\n",
      "Epoch 11, Loss: 233.5057\n",
      "Epoch 12, Loss: 233.0554\n",
      "Epoch 13, Loss: 232.6509\n",
      "Epoch 14, Loss: 232.3089\n",
      "Epoch 15, Loss: 232.0026\n",
      "Epoch 16, Loss: 231.7090\n",
      "Epoch 17, Loss: 231.4480\n",
      "Epoch 18, Loss: 231.2215\n",
      "Epoch 19, Loss: 231.0058\n",
      "Epoch 20, Loss: 230.7995\n",
      "Epoch 21, Loss: 230.6172\n",
      "Epoch 22, Loss: 230.4437\n",
      "Epoch 23, Loss: 230.2677\n",
      "Epoch 24, Loss: 230.1181\n",
      "Epoch 25, Loss: 229.9604\n",
      "Epoch 26, Loss: 229.8415\n",
      "Epoch 27, Loss: 229.6812\n",
      "Epoch 28, Loss: 229.5661\n",
      "Epoch 29, Loss: 229.4465\n",
      "Epoch 30, Loss: 229.3303\n",
      "Epoch 31, Loss: 229.2455\n",
      "Epoch 32, Loss: 229.1259\n",
      "Epoch 33, Loss: 229.0316\n",
      "Epoch 34, Loss: 228.9391\n",
      "Epoch 35, Loss: 228.8543\n",
      "Epoch 36, Loss: 228.7627\n",
      "Epoch 37, Loss: 228.6775\n",
      "Epoch 38, Loss: 228.5982\n",
      "Epoch 39, Loss: 228.5222\n",
      "Epoch 40, Loss: 228.4494\n",
      "Training completed in: 26.26 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    cvae.train()\n",
    "    train_loss = 0\n",
    "    for data, label, style in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = cvae(data, label, style)\n",
    "        loss = cvae_loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    print(f'Epoch {epoch + 1}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "torch.save(cvae.state_dict(), 'cvae.pth')\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "training_duration = end_time - start_time\n",
    "training_duration_minutes = training_duration / 60\n",
    "print(f\"Training completed in: {training_duration_minutes:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2d2c9dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 239.9837\n"
     ]
    }
   ],
   "source": [
    "cvae = CVAE(input_size, hidden_size, latent_dim, num_classes, num_styles)\n",
    "cvae.load_state_dict(torch.load('cvae.pth'))\n",
    "cvae.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for data, label, style in test_loader:\n",
    "        recon_batch, mu, logvar = cvae(data, label, style)\n",
    "        loss = cvae_loss_function(recon_batch, data, mu, logvar)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader.dataset)\n",
    "print(f'Test Loss: {avg_test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "ea95fbdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_images(model, class_label, style_label, num_images=5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_images, latent_dim)\n",
    "        c = torch.eye(num_classes)[torch.tensor([class_label] * num_images)]\n",
    "        s = torch.eye(num_styles)[torch.tensor([style_label] * num_images)]\n",
    "        generated_images = model.decode(z, c, s)\n",
    "        for i in range(num_images):\n",
    "            plt.imshow(generated_images[i].view(28, 28), cmap='gray')\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "generate_images(cvae, class_label=5, style_label=0, num_images=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74706a22",
   "metadata": {},
   "source": [
    "# => Conditional Generative Adversarial Network (C-GAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "2589551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, label_dim, style_dim, img_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.Linear(latent_dim + label_dim + style_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, img_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels, styles):\n",
    "        input = torch.cat((noise, labels, styles), dim=1)\n",
    "        return self.gen(input)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_dim, label_dim, style_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Linear(img_dim + label_dim + style_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels, styles):\n",
    "        input = torch.cat((img, labels, styles), dim=1)\n",
    "        return self.disc(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "1e5544b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "latent_dim = 128\n",
    "img_dim = 784  # 28x28 flattened\n",
    "label_dim = num_classes\n",
    "style_dim = num_styles\n",
    "batch_size = 64\n",
    "lr = 0.0001\n",
    "epochs = 40  \n",
    "\n",
    "# Initialize models\n",
    "generator = Generator(latent_dim, label_dim, style_dim, img_dim).to(device)\n",
    "discriminator = Discriminator(img_dim, label_dim, style_dim).to(device)\n",
    "\n",
    "# Optimizers\n",
    "opt_gen = optim.Adam(generator.parameters(), lr=lr)\n",
    "opt_disc = optim.Adam(discriminator.parameters(), lr=lr)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "save_generator = 'generator.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e688adc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/40] Batch 0/2324 Loss D: 0.6857, Loss G: 0.6943\n",
      "Epoch [0/40] Batch 100/2324 Loss D: 0.7644, Loss G: 0.6434\n",
      "Epoch [0/40] Batch 200/2324 Loss D: 0.5244, Loss G: 0.9300\n",
      "Epoch [0/40] Batch 300/2324 Loss D: 0.4958, Loss G: 1.2402\n",
      "Epoch [0/40] Batch 400/2324 Loss D: 0.6001, Loss G: 0.9966\n",
      "Epoch [0/40] Batch 500/2324 Loss D: 0.5586, Loss G: 0.9152\n",
      "Epoch [0/40] Batch 600/2324 Loss D: 0.4773, Loss G: 1.0519\n",
      "Epoch [0/40] Batch 700/2324 Loss D: 0.4181, Loss G: 1.0341\n",
      "Epoch [0/40] Batch 800/2324 Loss D: 0.4267, Loss G: 0.9842\n",
      "Epoch [0/40] Batch 900/2324 Loss D: 0.5051, Loss G: 0.9240\n",
      "Epoch [0/40] Batch 1000/2324 Loss D: 0.5522, Loss G: 0.7922\n",
      "Epoch [0/40] Batch 1100/2324 Loss D: 0.5399, Loss G: 0.8390\n",
      "Epoch [0/40] Batch 1200/2324 Loss D: 0.4885, Loss G: 0.9777\n",
      "Epoch [0/40] Batch 1300/2324 Loss D: 0.4065, Loss G: 1.2188\n",
      "Epoch [0/40] Batch 1400/2324 Loss D: 0.3885, Loss G: 1.2195\n",
      "Epoch [0/40] Batch 1500/2324 Loss D: 0.3270, Loss G: 1.3074\n",
      "Epoch [0/40] Batch 1600/2324 Loss D: 0.3921, Loss G: 1.1271\n",
      "Epoch [0/40] Batch 1700/2324 Loss D: 0.3693, Loss G: 1.1726\n",
      "Epoch [0/40] Batch 1800/2324 Loss D: 0.4349, Loss G: 1.0205\n",
      "Epoch [0/40] Batch 1900/2324 Loss D: 0.3877, Loss G: 1.1272\n",
      "Epoch [0/40] Batch 2000/2324 Loss D: 0.3881, Loss G: 1.2280\n",
      "Epoch [0/40] Batch 2100/2324 Loss D: 0.3504, Loss G: 1.3959\n",
      "Epoch [0/40] Batch 2200/2324 Loss D: 0.3367, Loss G: 1.4942\n",
      "Epoch [0/40] Batch 2300/2324 Loss D: 0.3768, Loss G: 1.3415\n",
      "Saved generator state dict to generator.pth after epoch 1\n",
      "Epoch [1/40] Batch 0/2324 Loss D: 0.3493, Loss G: 1.3894\n",
      "Epoch [1/40] Batch 100/2324 Loss D: 0.4075, Loss G: 1.2739\n",
      "Epoch [1/40] Batch 200/2324 Loss D: 0.3782, Loss G: 1.2717\n",
      "Epoch [1/40] Batch 300/2324 Loss D: 0.2867, Loss G: 1.5776\n",
      "Epoch [1/40] Batch 400/2324 Loss D: 0.3092, Loss G: 1.6256\n",
      "Epoch [1/40] Batch 500/2324 Loss D: 0.3124, Loss G: 1.6289\n",
      "Epoch [1/40] Batch 600/2324 Loss D: 0.2790, Loss G: 1.6524\n",
      "Epoch [1/40] Batch 700/2324 Loss D: 0.3097, Loss G: 1.6100\n",
      "Epoch [1/40] Batch 800/2324 Loss D: 0.3126, Loss G: 1.4776\n",
      "Epoch [1/40] Batch 900/2324 Loss D: 0.2879, Loss G: 1.5195\n",
      "Epoch [1/40] Batch 1000/2324 Loss D: 0.3145, Loss G: 1.6289\n",
      "Epoch [1/40] Batch 1100/2324 Loss D: 0.3194, Loss G: 1.5784\n",
      "Epoch [1/40] Batch 1200/2324 Loss D: 0.2517, Loss G: 1.9390\n",
      "Epoch [1/40] Batch 1300/2324 Loss D: 0.3077, Loss G: 1.6662\n",
      "Epoch [1/40] Batch 1400/2324 Loss D: 0.2102, Loss G: 2.1026\n",
      "Epoch [1/40] Batch 1500/2324 Loss D: 0.1944, Loss G: 2.0096\n",
      "Epoch [1/40] Batch 1600/2324 Loss D: 0.2851, Loss G: 1.6657\n",
      "Epoch [1/40] Batch 1700/2324 Loss D: 0.2343, Loss G: 1.7479\n",
      "Epoch [1/40] Batch 1800/2324 Loss D: 0.2252, Loss G: 1.8087\n",
      "Epoch [1/40] Batch 1900/2324 Loss D: 0.2281, Loss G: 1.9198\n",
      "Epoch [1/40] Batch 2000/2324 Loss D: 0.2664, Loss G: 1.8842\n",
      "Epoch [1/40] Batch 2100/2324 Loss D: 0.2393, Loss G: 1.9414\n",
      "Epoch [1/40] Batch 2200/2324 Loss D: 0.2851, Loss G: 1.7999\n",
      "Epoch [1/40] Batch 2300/2324 Loss D: 0.2019, Loss G: 1.8914\n",
      "Saved generator state dict to generator.pth after epoch 2\n",
      "Epoch [2/40] Batch 0/2324 Loss D: 0.2233, Loss G: 1.7278\n",
      "Epoch [2/40] Batch 100/2324 Loss D: 0.2071, Loss G: 2.0122\n",
      "Epoch [2/40] Batch 200/2324 Loss D: 0.2391, Loss G: 1.8224\n",
      "Epoch [2/40] Batch 300/2324 Loss D: 0.2113, Loss G: 2.0806\n",
      "Epoch [2/40] Batch 400/2324 Loss D: 0.2102, Loss G: 2.2006\n",
      "Epoch [2/40] Batch 500/2324 Loss D: 0.2125, Loss G: 2.1793\n",
      "Epoch [2/40] Batch 600/2324 Loss D: 0.2381, Loss G: 2.2014\n",
      "Epoch [2/40] Batch 700/2324 Loss D: 0.2265, Loss G: 1.9315\n",
      "Epoch [2/40] Batch 800/2324 Loss D: 0.2155, Loss G: 2.2312\n",
      "Epoch [2/40] Batch 900/2324 Loss D: 0.2119, Loss G: 2.1539\n",
      "Epoch [2/40] Batch 1000/2324 Loss D: 0.2098, Loss G: 1.9236\n",
      "Epoch [2/40] Batch 1100/2324 Loss D: 0.2597, Loss G: 1.9639\n",
      "Epoch [2/40] Batch 1200/2324 Loss D: 0.1835, Loss G: 2.1857\n",
      "Epoch [2/40] Batch 1300/2324 Loss D: 0.2048, Loss G: 2.1782\n",
      "Epoch [2/40] Batch 1400/2324 Loss D: 0.2228, Loss G: 2.2077\n",
      "Epoch [2/40] Batch 1500/2324 Loss D: 0.2166, Loss G: 2.3244\n",
      "Epoch [2/40] Batch 1600/2324 Loss D: 0.2399, Loss G: 1.9303\n",
      "Epoch [2/40] Batch 1700/2324 Loss D: 0.2271, Loss G: 2.0406\n",
      "Epoch [2/40] Batch 1800/2324 Loss D: 0.1679, Loss G: 2.2630\n",
      "Epoch [2/40] Batch 1900/2324 Loss D: 0.2034, Loss G: 2.2296\n",
      "Epoch [2/40] Batch 2000/2324 Loss D: 0.2163, Loss G: 2.0510\n",
      "Epoch [2/40] Batch 2100/2324 Loss D: 0.1967, Loss G: 2.4450\n",
      "Epoch [2/40] Batch 2200/2324 Loss D: 0.2740, Loss G: 1.9086\n",
      "Epoch [2/40] Batch 2300/2324 Loss D: 0.2110, Loss G: 2.4135\n",
      "Saved generator state dict to generator.pth after epoch 3\n",
      "Epoch [3/40] Batch 0/2324 Loss D: 0.1884, Loss G: 2.3215\n",
      "Epoch [3/40] Batch 100/2324 Loss D: 0.2144, Loss G: 2.1847\n",
      "Epoch [3/40] Batch 200/2324 Loss D: 0.2694, Loss G: 1.9169\n",
      "Epoch [3/40] Batch 300/2324 Loss D: 0.2075, Loss G: 2.1529\n",
      "Epoch [3/40] Batch 400/2324 Loss D: 0.2243, Loss G: 2.0582\n",
      "Epoch [3/40] Batch 500/2324 Loss D: 0.2187, Loss G: 2.0791\n",
      "Epoch [3/40] Batch 600/2324 Loss D: 0.2948, Loss G: 1.9655\n",
      "Epoch [3/40] Batch 700/2324 Loss D: 0.2073, Loss G: 2.1568\n",
      "Epoch [3/40] Batch 800/2324 Loss D: 0.2200, Loss G: 2.0644\n",
      "Epoch [3/40] Batch 900/2324 Loss D: 0.2199, Loss G: 2.0513\n",
      "Epoch [3/40] Batch 1000/2324 Loss D: 0.2099, Loss G: 2.1308\n",
      "Epoch [3/40] Batch 1100/2324 Loss D: 0.2596, Loss G: 2.3170\n",
      "Epoch [3/40] Batch 1200/2324 Loss D: 0.2485, Loss G: 1.9735\n",
      "Epoch [3/40] Batch 1300/2324 Loss D: 0.1983, Loss G: 2.1761\n",
      "Epoch [3/40] Batch 1400/2324 Loss D: 0.2069, Loss G: 2.2595\n",
      "Epoch [3/40] Batch 1500/2324 Loss D: 0.2618, Loss G: 1.9703\n",
      "Epoch [3/40] Batch 1600/2324 Loss D: 0.2172, Loss G: 2.0477\n",
      "Epoch [3/40] Batch 1700/2324 Loss D: 0.2196, Loss G: 2.1618\n",
      "Epoch [3/40] Batch 1800/2324 Loss D: 0.2051, Loss G: 2.1066\n",
      "Epoch [3/40] Batch 1900/2324 Loss D: 0.1947, Loss G: 2.2058\n",
      "Epoch [3/40] Batch 2000/2324 Loss D: 0.2334, Loss G: 2.1289\n",
      "Epoch [3/40] Batch 2100/2324 Loss D: 0.1677, Loss G: 2.5268\n",
      "Epoch [3/40] Batch 2200/2324 Loss D: 0.2157, Loss G: 2.3306\n",
      "Epoch [3/40] Batch 2300/2324 Loss D: 0.2138, Loss G: 2.0672\n",
      "Saved generator state dict to generator.pth after epoch 4\n",
      "Epoch [4/40] Batch 0/2324 Loss D: 0.1990, Loss G: 2.2260\n",
      "Epoch [4/40] Batch 100/2324 Loss D: 0.1993, Loss G: 2.2801\n",
      "Epoch [4/40] Batch 200/2324 Loss D: 0.2341, Loss G: 2.0109\n",
      "Epoch [4/40] Batch 300/2324 Loss D: 0.1852, Loss G: 2.3622\n",
      "Epoch [4/40] Batch 400/2324 Loss D: 0.2322, Loss G: 2.3210\n",
      "Epoch [4/40] Batch 500/2324 Loss D: 0.2103, Loss G: 2.3339\n",
      "Epoch [4/40] Batch 600/2324 Loss D: 0.1792, Loss G: 2.4538\n",
      "Epoch [4/40] Batch 700/2324 Loss D: 0.1941, Loss G: 2.2119\n",
      "Epoch [4/40] Batch 800/2324 Loss D: 0.2261, Loss G: 2.2078\n",
      "Epoch [4/40] Batch 900/2324 Loss D: 0.2392, Loss G: 1.9224\n",
      "Epoch [4/40] Batch 1000/2324 Loss D: 0.2208, Loss G: 1.9739\n",
      "Epoch [4/40] Batch 1100/2324 Loss D: 0.1701, Loss G: 2.3722\n",
      "Epoch [4/40] Batch 1200/2324 Loss D: 0.1981, Loss G: 2.3425\n",
      "Epoch [4/40] Batch 1300/2324 Loss D: 0.1863, Loss G: 2.5166\n",
      "Epoch [4/40] Batch 1400/2324 Loss D: 0.1923, Loss G: 2.3496\n",
      "Epoch [4/40] Batch 1500/2324 Loss D: 0.1664, Loss G: 2.4851\n",
      "Epoch [4/40] Batch 1600/2324 Loss D: 0.1876, Loss G: 2.2913\n",
      "Epoch [4/40] Batch 1700/2324 Loss D: 0.2511, Loss G: 2.0466\n",
      "Epoch [4/40] Batch 1800/2324 Loss D: 0.1874, Loss G: 2.1089\n",
      "Epoch [4/40] Batch 1900/2324 Loss D: 0.2451, Loss G: 2.0368\n",
      "Epoch [4/40] Batch 2000/2324 Loss D: 0.2257, Loss G: 2.2540\n",
      "Epoch [4/40] Batch 2100/2324 Loss D: 0.2702, Loss G: 2.2852\n",
      "Epoch [4/40] Batch 2200/2324 Loss D: 0.1870, Loss G: 2.4477\n",
      "Epoch [4/40] Batch 2300/2324 Loss D: 0.1514, Loss G: 2.5126\n",
      "Saved generator state dict to generator.pth after epoch 5\n",
      "Epoch [5/40] Batch 0/2324 Loss D: 0.2047, Loss G: 2.3001\n",
      "Epoch [5/40] Batch 100/2324 Loss D: 0.2502, Loss G: 2.1565\n",
      "Epoch [5/40] Batch 200/2324 Loss D: 0.1878, Loss G: 2.4048\n",
      "Epoch [5/40] Batch 300/2324 Loss D: 0.2385, Loss G: 2.3575\n",
      "Epoch [5/40] Batch 400/2324 Loss D: 0.2253, Loss G: 2.1747\n",
      "Epoch [5/40] Batch 500/2324 Loss D: 0.2448, Loss G: 2.2821\n",
      "Epoch [5/40] Batch 600/2324 Loss D: 0.2262, Loss G: 2.1654\n",
      "Epoch [5/40] Batch 700/2324 Loss D: 0.1988, Loss G: 2.5150\n",
      "Epoch [5/40] Batch 800/2324 Loss D: 0.1814, Loss G: 2.3342\n",
      "Epoch [5/40] Batch 900/2324 Loss D: 0.1898, Loss G: 2.2456\n",
      "Epoch [5/40] Batch 1000/2324 Loss D: 0.2601, Loss G: 2.2399\n",
      "Epoch [5/40] Batch 1100/2324 Loss D: 0.1966, Loss G: 2.3009\n",
      "Epoch [5/40] Batch 1200/2324 Loss D: 0.2137, Loss G: 2.2931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/40] Batch 1300/2324 Loss D: 0.2228, Loss G: 2.1325\n",
      "Epoch [5/40] Batch 1400/2324 Loss D: 0.2019, Loss G: 2.3529\n",
      "Epoch [5/40] Batch 1500/2324 Loss D: 0.2084, Loss G: 2.3265\n",
      "Epoch [5/40] Batch 1600/2324 Loss D: 0.2130, Loss G: 2.2222\n",
      "Epoch [5/40] Batch 1700/2324 Loss D: 0.2061, Loss G: 2.4802\n",
      "Epoch [5/40] Batch 1800/2324 Loss D: 0.2164, Loss G: 2.2607\n",
      "Epoch [5/40] Batch 1900/2324 Loss D: 0.2530, Loss G: 2.1962\n",
      "Epoch [5/40] Batch 2000/2324 Loss D: 0.2212, Loss G: 2.4400\n",
      "Epoch [5/40] Batch 2100/2324 Loss D: 0.2992, Loss G: 2.0933\n",
      "Epoch [5/40] Batch 2200/2324 Loss D: 0.2417, Loss G: 2.4590\n",
      "Epoch [5/40] Batch 2300/2324 Loss D: 0.2076, Loss G: 2.4692\n",
      "Saved generator state dict to generator.pth after epoch 6\n",
      "Epoch [6/40] Batch 0/2324 Loss D: 0.1606, Loss G: 2.4119\n",
      "Epoch [6/40] Batch 100/2324 Loss D: 0.2302, Loss G: 2.3841\n",
      "Epoch [6/40] Batch 200/2324 Loss D: 0.1967, Loss G: 2.5358\n",
      "Epoch [6/40] Batch 300/2324 Loss D: 0.1951, Loss G: 2.3566\n",
      "Epoch [6/40] Batch 400/2324 Loss D: 0.2388, Loss G: 2.2044\n",
      "Epoch [6/40] Batch 500/2324 Loss D: 0.2325, Loss G: 2.4307\n",
      "Epoch [6/40] Batch 600/2324 Loss D: 0.2206, Loss G: 2.3630\n",
      "Epoch [6/40] Batch 700/2324 Loss D: 0.2604, Loss G: 2.2874\n",
      "Epoch [6/40] Batch 800/2324 Loss D: 0.2182, Loss G: 2.3444\n",
      "Epoch [6/40] Batch 900/2324 Loss D: 0.1962, Loss G: 2.4410\n",
      "Epoch [6/40] Batch 1000/2324 Loss D: 0.1984, Loss G: 2.3670\n",
      "Epoch [6/40] Batch 1100/2324 Loss D: 0.1587, Loss G: 2.5883\n",
      "Epoch [6/40] Batch 1200/2324 Loss D: 0.1889, Loss G: 2.4244\n",
      "Epoch [6/40] Batch 1300/2324 Loss D: 0.2178, Loss G: 2.2546\n",
      "Epoch [6/40] Batch 1400/2324 Loss D: 0.2143, Loss G: 2.3079\n",
      "Epoch [6/40] Batch 1500/2324 Loss D: 0.2916, Loss G: 2.5260\n",
      "Epoch [6/40] Batch 1600/2324 Loss D: 0.2120, Loss G: 2.2535\n",
      "Epoch [6/40] Batch 1700/2324 Loss D: 0.2073, Loss G: 2.3157\n",
      "Epoch [6/40] Batch 1800/2324 Loss D: 0.2194, Loss G: 2.2285\n",
      "Epoch [6/40] Batch 1900/2324 Loss D: 0.2548, Loss G: 2.1827\n",
      "Epoch [6/40] Batch 2000/2324 Loss D: 0.2383, Loss G: 2.4144\n",
      "Epoch [6/40] Batch 2100/2324 Loss D: 0.2335, Loss G: 2.1625\n",
      "Epoch [6/40] Batch 2200/2324 Loss D: 0.1592, Loss G: 2.6145\n",
      "Epoch [6/40] Batch 2300/2324 Loss D: 0.2078, Loss G: 2.4520\n",
      "Saved generator state dict to generator.pth after epoch 7\n",
      "Epoch [7/40] Batch 0/2324 Loss D: 0.2182, Loss G: 2.1727\n",
      "Epoch [7/40] Batch 100/2324 Loss D: 0.2472, Loss G: 2.4145\n",
      "Epoch [7/40] Batch 200/2324 Loss D: 0.2038, Loss G: 2.3286\n",
      "Epoch [7/40] Batch 300/2324 Loss D: 0.2351, Loss G: 2.2446\n",
      "Epoch [7/40] Batch 400/2324 Loss D: 0.2140, Loss G: 2.4913\n",
      "Epoch [7/40] Batch 500/2324 Loss D: 0.2106, Loss G: 2.4432\n",
      "Epoch [7/40] Batch 600/2324 Loss D: 0.1708, Loss G: 2.3221\n",
      "Epoch [7/40] Batch 700/2324 Loss D: 0.1536, Loss G: 2.4382\n",
      "Epoch [7/40] Batch 800/2324 Loss D: 0.2673, Loss G: 2.1062\n",
      "Epoch [7/40] Batch 900/2324 Loss D: 0.2330, Loss G: 2.1345\n",
      "Epoch [7/40] Batch 1000/2324 Loss D: 0.1843, Loss G: 2.6502\n",
      "Epoch [7/40] Batch 1100/2324 Loss D: 0.2320, Loss G: 2.2734\n",
      "Epoch [7/40] Batch 1200/2324 Loss D: 0.1590, Loss G: 2.5193\n",
      "Epoch [7/40] Batch 1300/2324 Loss D: 0.1612, Loss G: 2.5712\n",
      "Epoch [7/40] Batch 1400/2324 Loss D: 0.2523, Loss G: 2.4951\n",
      "Epoch [7/40] Batch 1500/2324 Loss D: 0.1863, Loss G: 2.5842\n",
      "Epoch [7/40] Batch 1600/2324 Loss D: 0.1860, Loss G: 2.5749\n",
      "Epoch [7/40] Batch 1700/2324 Loss D: 0.2282, Loss G: 2.3220\n",
      "Epoch [7/40] Batch 1800/2324 Loss D: 0.1959, Loss G: 2.2599\n",
      "Epoch [7/40] Batch 1900/2324 Loss D: 0.2239, Loss G: 2.3579\n",
      "Epoch [7/40] Batch 2000/2324 Loss D: 0.2223, Loss G: 2.1975\n",
      "Epoch [7/40] Batch 2100/2324 Loss D: 0.2035, Loss G: 2.4962\n",
      "Epoch [7/40] Batch 2200/2324 Loss D: 0.1617, Loss G: 2.5318\n",
      "Epoch [7/40] Batch 2300/2324 Loss D: 0.2255, Loss G: 2.5731\n",
      "Saved generator state dict to generator.pth after epoch 8\n",
      "Epoch [8/40] Batch 0/2324 Loss D: 0.2102, Loss G: 2.4489\n",
      "Epoch [8/40] Batch 100/2324 Loss D: 0.1738, Loss G: 2.4448\n",
      "Epoch [8/40] Batch 200/2324 Loss D: 0.2147, Loss G: 2.3182\n",
      "Epoch [8/40] Batch 300/2324 Loss D: 0.2512, Loss G: 2.1671\n",
      "Epoch [8/40] Batch 400/2324 Loss D: 0.2214, Loss G: 2.5764\n",
      "Epoch [8/40] Batch 500/2324 Loss D: 0.2228, Loss G: 2.4079\n",
      "Epoch [8/40] Batch 600/2324 Loss D: 0.1830, Loss G: 2.6355\n",
      "Epoch [8/40] Batch 700/2324 Loss D: 0.2559, Loss G: 2.2308\n",
      "Epoch [8/40] Batch 800/2324 Loss D: 0.2059, Loss G: 2.1654\n",
      "Epoch [8/40] Batch 900/2324 Loss D: 0.1709, Loss G: 2.6493\n",
      "Epoch [8/40] Batch 1000/2324 Loss D: 0.2294, Loss G: 2.3265\n",
      "Epoch [8/40] Batch 1100/2324 Loss D: 0.2098, Loss G: 2.5395\n",
      "Epoch [8/40] Batch 1200/2324 Loss D: 0.2451, Loss G: 2.2908\n",
      "Epoch [8/40] Batch 1300/2324 Loss D: 0.2015, Loss G: 2.3170\n",
      "Epoch [8/40] Batch 1400/2324 Loss D: 0.2051, Loss G: 2.2662\n",
      "Epoch [8/40] Batch 1500/2324 Loss D: 0.2300, Loss G: 2.3836\n",
      "Epoch [8/40] Batch 1600/2324 Loss D: 0.2044, Loss G: 2.3182\n",
      "Epoch [8/40] Batch 1700/2324 Loss D: 0.1884, Loss G: 2.5531\n",
      "Epoch [8/40] Batch 1800/2324 Loss D: 0.1713, Loss G: 2.4252\n",
      "Epoch [8/40] Batch 1900/2324 Loss D: 0.1996, Loss G: 2.3846\n",
      "Epoch [8/40] Batch 2000/2324 Loss D: 0.1514, Loss G: 2.5536\n",
      "Epoch [8/40] Batch 2100/2324 Loss D: 0.2004, Loss G: 2.5794\n",
      "Epoch [8/40] Batch 2200/2324 Loss D: 0.2160, Loss G: 2.2894\n",
      "Epoch [8/40] Batch 2300/2324 Loss D: 0.2196, Loss G: 2.5578\n",
      "Saved generator state dict to generator.pth after epoch 9\n",
      "Epoch [9/40] Batch 0/2324 Loss D: 0.2306, Loss G: 2.5092\n",
      "Epoch [9/40] Batch 100/2324 Loss D: 0.2328, Loss G: 2.2861\n",
      "Epoch [9/40] Batch 200/2324 Loss D: 0.1737, Loss G: 2.4934\n",
      "Epoch [9/40] Batch 300/2324 Loss D: 0.2081, Loss G: 2.2449\n",
      "Epoch [9/40] Batch 400/2324 Loss D: 0.1798, Loss G: 2.4811\n",
      "Epoch [9/40] Batch 500/2324 Loss D: 0.1519, Loss G: 2.7060\n",
      "Epoch [9/40] Batch 600/2324 Loss D: 0.1433, Loss G: 2.8847\n",
      "Epoch [9/40] Batch 700/2324 Loss D: 0.1677, Loss G: 2.8134\n",
      "Epoch [9/40] Batch 800/2324 Loss D: 0.1968, Loss G: 2.4589\n",
      "Epoch [9/40] Batch 900/2324 Loss D: 0.1980, Loss G: 2.4712\n",
      "Epoch [9/40] Batch 1000/2324 Loss D: 0.2692, Loss G: 2.1294\n",
      "Epoch [9/40] Batch 1100/2324 Loss D: 0.1940, Loss G: 2.6557\n",
      "Epoch [9/40] Batch 1200/2324 Loss D: 0.2063, Loss G: 2.3594\n",
      "Epoch [9/40] Batch 1300/2324 Loss D: 0.1971, Loss G: 2.6822\n",
      "Epoch [9/40] Batch 1400/2324 Loss D: 0.1480, Loss G: 2.6507\n",
      "Epoch [9/40] Batch 1500/2324 Loss D: 0.1985, Loss G: 2.5293\n",
      "Epoch [9/40] Batch 1600/2324 Loss D: 0.1519, Loss G: 2.7178\n",
      "Epoch [9/40] Batch 1700/2324 Loss D: 0.2186, Loss G: 2.5504\n",
      "Epoch [9/40] Batch 1800/2324 Loss D: 0.1628, Loss G: 2.6937\n",
      "Epoch [9/40] Batch 1900/2324 Loss D: 0.1985, Loss G: 2.3962\n",
      "Epoch [9/40] Batch 2000/2324 Loss D: 0.1993, Loss G: 2.2658\n",
      "Epoch [9/40] Batch 2100/2324 Loss D: 0.1787, Loss G: 2.6649\n",
      "Epoch [9/40] Batch 2200/2324 Loss D: 0.1901, Loss G: 2.4838\n",
      "Epoch [9/40] Batch 2300/2324 Loss D: 0.1936, Loss G: 2.5932\n",
      "Saved generator state dict to generator.pth after epoch 10\n",
      "Epoch [10/40] Batch 0/2324 Loss D: 0.2482, Loss G: 2.4286\n",
      "Epoch [10/40] Batch 100/2324 Loss D: 0.1360, Loss G: 2.8181\n",
      "Epoch [10/40] Batch 200/2324 Loss D: 0.2078, Loss G: 2.5455\n",
      "Epoch [10/40] Batch 300/2324 Loss D: 0.1480, Loss G: 2.9740\n",
      "Epoch [10/40] Batch 400/2324 Loss D: 0.1891, Loss G: 2.6579\n",
      "Epoch [10/40] Batch 500/2324 Loss D: 0.1668, Loss G: 2.7944\n",
      "Epoch [10/40] Batch 600/2324 Loss D: 0.1752, Loss G: 2.7055\n",
      "Epoch [10/40] Batch 700/2324 Loss D: 0.2004, Loss G: 2.6524\n",
      "Epoch [10/40] Batch 800/2324 Loss D: 0.2158, Loss G: 2.4911\n",
      "Epoch [10/40] Batch 900/2324 Loss D: 0.1961, Loss G: 2.6164\n",
      "Epoch [10/40] Batch 1000/2324 Loss D: 0.1614, Loss G: 2.7632\n",
      "Epoch [10/40] Batch 1100/2324 Loss D: 0.1861, Loss G: 2.6356\n",
      "Epoch [10/40] Batch 1200/2324 Loss D: 0.1610, Loss G: 2.7866\n",
      "Epoch [10/40] Batch 1300/2324 Loss D: 0.2063, Loss G: 2.8199\n",
      "Epoch [10/40] Batch 1400/2324 Loss D: 0.1747, Loss G: 2.6148\n",
      "Epoch [10/40] Batch 1500/2324 Loss D: 0.1764, Loss G: 2.7193\n",
      "Epoch [10/40] Batch 1600/2324 Loss D: 0.2019, Loss G: 2.4704\n",
      "Epoch [10/40] Batch 1700/2324 Loss D: 0.1999, Loss G: 2.4579\n",
      "Epoch [10/40] Batch 1800/2324 Loss D: 0.1982, Loss G: 2.4765\n",
      "Epoch [10/40] Batch 1900/2324 Loss D: 0.2067, Loss G: 2.4653\n",
      "Epoch [10/40] Batch 2000/2324 Loss D: 0.2055, Loss G: 2.4588\n",
      "Epoch [10/40] Batch 2100/2324 Loss D: 0.1552, Loss G: 2.6996\n",
      "Epoch [10/40] Batch 2200/2324 Loss D: 0.1727, Loss G: 2.6676\n",
      "Epoch [10/40] Batch 2300/2324 Loss D: 0.1919, Loss G: 3.0126\n",
      "Saved generator state dict to generator.pth after epoch 11\n",
      "Epoch [11/40] Batch 0/2324 Loss D: 0.1557, Loss G: 2.8725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/40] Batch 100/2324 Loss D: 0.1543, Loss G: 2.6978\n",
      "Epoch [11/40] Batch 200/2324 Loss D: 0.1769, Loss G: 2.8437\n",
      "Epoch [11/40] Batch 300/2324 Loss D: 0.1556, Loss G: 2.8132\n",
      "Epoch [11/40] Batch 400/2324 Loss D: 0.1899, Loss G: 2.6893\n",
      "Epoch [11/40] Batch 500/2324 Loss D: 0.2009, Loss G: 2.6937\n",
      "Epoch [11/40] Batch 600/2324 Loss D: 0.1611, Loss G: 2.6204\n",
      "Epoch [11/40] Batch 700/2324 Loss D: 0.2044, Loss G: 2.7712\n",
      "Epoch [11/40] Batch 800/2324 Loss D: 0.1894, Loss G: 2.6074\n",
      "Epoch [11/40] Batch 900/2324 Loss D: 0.1617, Loss G: 3.0023\n",
      "Epoch [11/40] Batch 1000/2324 Loss D: 0.1972, Loss G: 2.8388\n",
      "Epoch [11/40] Batch 1100/2324 Loss D: 0.1938, Loss G: 2.9947\n",
      "Epoch [11/40] Batch 1200/2324 Loss D: 0.1752, Loss G: 2.6978\n",
      "Epoch [11/40] Batch 1300/2324 Loss D: 0.2167, Loss G: 2.3925\n",
      "Epoch [11/40] Batch 1400/2324 Loss D: 0.1881, Loss G: 2.5697\n",
      "Epoch [11/40] Batch 1500/2324 Loss D: 0.2278, Loss G: 2.5730\n",
      "Epoch [11/40] Batch 1600/2324 Loss D: 0.1773, Loss G: 2.8682\n",
      "Epoch [11/40] Batch 1700/2324 Loss D: 0.2165, Loss G: 2.5944\n",
      "Epoch [11/40] Batch 1800/2324 Loss D: 0.2020, Loss G: 2.5256\n",
      "Epoch [11/40] Batch 1900/2324 Loss D: 0.1313, Loss G: 2.9128\n",
      "Epoch [11/40] Batch 2000/2324 Loss D: 0.1982, Loss G: 2.5499\n",
      "Epoch [11/40] Batch 2100/2324 Loss D: 0.1579, Loss G: 2.7907\n",
      "Epoch [11/40] Batch 2200/2324 Loss D: 0.1636, Loss G: 2.5813\n",
      "Epoch [11/40] Batch 2300/2324 Loss D: 0.1464, Loss G: 2.8250\n",
      "Saved generator state dict to generator.pth after epoch 12\n",
      "Epoch [12/40] Batch 0/2324 Loss D: 0.1872, Loss G: 2.6998\n",
      "Epoch [12/40] Batch 100/2324 Loss D: 0.1785, Loss G: 2.5787\n",
      "Epoch [12/40] Batch 200/2324 Loss D: 0.1593, Loss G: 2.7407\n",
      "Epoch [12/40] Batch 300/2324 Loss D: 0.2079, Loss G: 2.4024\n",
      "Epoch [12/40] Batch 400/2324 Loss D: 0.2034, Loss G: 2.8732\n",
      "Epoch [12/40] Batch 500/2324 Loss D: 0.2278, Loss G: 2.7091\n",
      "Epoch [12/40] Batch 600/2324 Loss D: 0.2053, Loss G: 2.8087\n",
      "Epoch [12/40] Batch 700/2324 Loss D: 0.1813, Loss G: 2.7368\n",
      "Epoch [12/40] Batch 800/2324 Loss D: 0.2044, Loss G: 2.6114\n",
      "Epoch [12/40] Batch 900/2324 Loss D: 0.1327, Loss G: 3.0117\n",
      "Epoch [12/40] Batch 1000/2324 Loss D: 0.2144, Loss G: 2.6792\n",
      "Epoch [12/40] Batch 1100/2324 Loss D: 0.1808, Loss G: 2.8341\n",
      "Epoch [12/40] Batch 1200/2324 Loss D: 0.1734, Loss G: 2.5644\n",
      "Epoch [12/40] Batch 1300/2324 Loss D: 0.1469, Loss G: 3.0041\n",
      "Epoch [12/40] Batch 1400/2324 Loss D: 0.1881, Loss G: 2.7857\n",
      "Epoch [12/40] Batch 1500/2324 Loss D: 0.2102, Loss G: 3.0022\n",
      "Epoch [12/40] Batch 1600/2324 Loss D: 0.1984, Loss G: 2.5011\n",
      "Epoch [12/40] Batch 1700/2324 Loss D: 0.1932, Loss G: 2.6658\n",
      "Epoch [12/40] Batch 1800/2324 Loss D: 0.1763, Loss G: 2.7264\n",
      "Epoch [12/40] Batch 1900/2324 Loss D: 0.1655, Loss G: 3.0952\n",
      "Epoch [12/40] Batch 2000/2324 Loss D: 0.2195, Loss G: 2.6611\n",
      "Epoch [12/40] Batch 2100/2324 Loss D: 0.1580, Loss G: 2.9915\n",
      "Epoch [12/40] Batch 2200/2324 Loss D: 0.1452, Loss G: 2.7397\n",
      "Epoch [12/40] Batch 2300/2324 Loss D: 0.2385, Loss G: 2.8526\n",
      "Saved generator state dict to generator.pth after epoch 13\n",
      "Epoch [13/40] Batch 0/2324 Loss D: 0.1436, Loss G: 3.0016\n",
      "Epoch [13/40] Batch 100/2324 Loss D: 0.2021, Loss G: 2.6782\n",
      "Epoch [13/40] Batch 200/2324 Loss D: 0.1366, Loss G: 2.7992\n",
      "Epoch [13/40] Batch 300/2324 Loss D: 0.1811, Loss G: 2.8161\n",
      "Epoch [13/40] Batch 400/2324 Loss D: 0.1545, Loss G: 2.9416\n",
      "Epoch [13/40] Batch 500/2324 Loss D: 0.1933, Loss G: 2.9708\n",
      "Epoch [13/40] Batch 600/2324 Loss D: 0.1668, Loss G: 2.7884\n",
      "Epoch [13/40] Batch 700/2324 Loss D: 0.1577, Loss G: 2.9434\n",
      "Epoch [13/40] Batch 800/2324 Loss D: 0.1786, Loss G: 2.7856\n",
      "Epoch [13/40] Batch 900/2324 Loss D: 0.1733, Loss G: 2.6953\n",
      "Epoch [13/40] Batch 1000/2324 Loss D: 0.2384, Loss G: 2.6074\n",
      "Epoch [13/40] Batch 1100/2324 Loss D: 0.1905, Loss G: 3.0043\n",
      "Epoch [13/40] Batch 1200/2324 Loss D: 0.2285, Loss G: 2.3899\n",
      "Epoch [13/40] Batch 1300/2324 Loss D: 0.2003, Loss G: 2.6459\n",
      "Epoch [13/40] Batch 1400/2324 Loss D: 0.2386, Loss G: 2.6160\n",
      "Epoch [13/40] Batch 1500/2324 Loss D: 0.2060, Loss G: 2.8196\n",
      "Epoch [13/40] Batch 1600/2324 Loss D: 0.1612, Loss G: 2.8454\n",
      "Epoch [13/40] Batch 1700/2324 Loss D: 0.1808, Loss G: 2.7160\n",
      "Epoch [13/40] Batch 1800/2324 Loss D: 0.1803, Loss G: 2.8602\n",
      "Epoch [13/40] Batch 1900/2324 Loss D: 0.1669, Loss G: 3.1700\n",
      "Epoch [13/40] Batch 2000/2324 Loss D: 0.1785, Loss G: 3.0766\n",
      "Epoch [13/40] Batch 2100/2324 Loss D: 0.1803, Loss G: 2.8281\n",
      "Epoch [13/40] Batch 2200/2324 Loss D: 0.1728, Loss G: 2.7404\n",
      "Epoch [13/40] Batch 2300/2324 Loss D: 0.2098, Loss G: 2.6510\n",
      "Saved generator state dict to generator.pth after epoch 14\n",
      "Epoch [14/40] Batch 0/2324 Loss D: 0.2069, Loss G: 2.7150\n",
      "Epoch [14/40] Batch 100/2324 Loss D: 0.1581, Loss G: 2.6603\n",
      "Epoch [14/40] Batch 200/2324 Loss D: 0.2052, Loss G: 2.8359\n",
      "Epoch [14/40] Batch 300/2324 Loss D: 0.1817, Loss G: 2.5987\n",
      "Epoch [14/40] Batch 400/2324 Loss D: 0.1564, Loss G: 3.1164\n",
      "Epoch [14/40] Batch 500/2324 Loss D: 0.2208, Loss G: 2.8871\n",
      "Epoch [14/40] Batch 600/2324 Loss D: 0.1752, Loss G: 2.6269\n",
      "Epoch [14/40] Batch 700/2324 Loss D: 0.1948, Loss G: 2.9164\n",
      "Epoch [14/40] Batch 800/2324 Loss D: 0.2099, Loss G: 2.8910\n",
      "Epoch [14/40] Batch 900/2324 Loss D: 0.1767, Loss G: 2.7222\n",
      "Epoch [14/40] Batch 1000/2324 Loss D: 0.1957, Loss G: 2.7542\n",
      "Epoch [14/40] Batch 1100/2324 Loss D: 0.1482, Loss G: 2.7277\n",
      "Epoch [14/40] Batch 1200/2324 Loss D: 0.1781, Loss G: 2.4403\n",
      "Epoch [14/40] Batch 1300/2324 Loss D: 0.2368, Loss G: 2.6784\n",
      "Epoch [14/40] Batch 1400/2324 Loss D: 0.1735, Loss G: 2.8625\n",
      "Epoch [14/40] Batch 1500/2324 Loss D: 0.1654, Loss G: 2.7371\n",
      "Epoch [14/40] Batch 1600/2324 Loss D: 0.1439, Loss G: 2.7832\n",
      "Epoch [14/40] Batch 1700/2324 Loss D: 0.1492, Loss G: 2.9748\n",
      "Epoch [14/40] Batch 1800/2324 Loss D: 0.2068, Loss G: 2.5825\n",
      "Epoch [14/40] Batch 1900/2324 Loss D: 0.1720, Loss G: 2.8669\n",
      "Epoch [14/40] Batch 2000/2324 Loss D: 0.1508, Loss G: 2.8567\n",
      "Epoch [14/40] Batch 2100/2324 Loss D: 0.1663, Loss G: 2.8364\n",
      "Epoch [14/40] Batch 2200/2324 Loss D: 0.1944, Loss G: 2.7396\n",
      "Epoch [14/40] Batch 2300/2324 Loss D: 0.2275, Loss G: 2.5020\n",
      "Saved generator state dict to generator.pth after epoch 15\n",
      "Epoch [15/40] Batch 0/2324 Loss D: 0.2014, Loss G: 2.7989\n",
      "Epoch [15/40] Batch 100/2324 Loss D: 0.1685, Loss G: 2.6208\n",
      "Epoch [15/40] Batch 200/2324 Loss D: 0.2093, Loss G: 2.7378\n",
      "Epoch [15/40] Batch 300/2324 Loss D: 0.1671, Loss G: 2.8273\n",
      "Epoch [15/40] Batch 400/2324 Loss D: 0.2242, Loss G: 2.5453\n",
      "Epoch [15/40] Batch 500/2324 Loss D: 0.1845, Loss G: 2.6839\n",
      "Epoch [15/40] Batch 600/2324 Loss D: 0.1685, Loss G: 2.6792\n",
      "Epoch [15/40] Batch 700/2324 Loss D: 0.2509, Loss G: 2.7949\n",
      "Epoch [15/40] Batch 800/2324 Loss D: 0.1417, Loss G: 3.0679\n",
      "Epoch [15/40] Batch 900/2324 Loss D: 0.2048, Loss G: 2.7886\n",
      "Epoch [15/40] Batch 1000/2324 Loss D: 0.2528, Loss G: 2.3918\n",
      "Epoch [15/40] Batch 1100/2324 Loss D: 0.2133, Loss G: 2.4713\n",
      "Epoch [15/40] Batch 1200/2324 Loss D: 0.1752, Loss G: 2.7203\n",
      "Epoch [15/40] Batch 1300/2324 Loss D: 0.1763, Loss G: 2.8888\n",
      "Epoch [15/40] Batch 1400/2324 Loss D: 0.1806, Loss G: 2.8915\n",
      "Epoch [15/40] Batch 1500/2324 Loss D: 0.2265, Loss G: 2.5923\n",
      "Epoch [15/40] Batch 1600/2324 Loss D: 0.3126, Loss G: 2.5462\n",
      "Epoch [15/40] Batch 1700/2324 Loss D: 0.2050, Loss G: 2.4581\n",
      "Epoch [15/40] Batch 1800/2324 Loss D: 0.1702, Loss G: 2.9055\n",
      "Epoch [15/40] Batch 1900/2324 Loss D: 0.1963, Loss G: 2.5949\n",
      "Epoch [15/40] Batch 2000/2324 Loss D: 0.1890, Loss G: 2.8257\n",
      "Epoch [15/40] Batch 2100/2324 Loss D: 0.1907, Loss G: 2.8841\n",
      "Epoch [15/40] Batch 2200/2324 Loss D: 0.2071, Loss G: 2.7177\n",
      "Epoch [15/40] Batch 2300/2324 Loss D: 0.2037, Loss G: 2.8322\n",
      "Saved generator state dict to generator.pth after epoch 16\n",
      "Epoch [16/40] Batch 0/2324 Loss D: 0.2440, Loss G: 2.7028\n",
      "Epoch [16/40] Batch 100/2324 Loss D: 0.2198, Loss G: 2.9426\n",
      "Epoch [16/40] Batch 200/2324 Loss D: 0.1603, Loss G: 2.8015\n",
      "Epoch [16/40] Batch 300/2324 Loss D: 0.2224, Loss G: 2.4430\n",
      "Epoch [16/40] Batch 400/2324 Loss D: 0.2275, Loss G: 2.4712\n",
      "Epoch [16/40] Batch 500/2324 Loss D: 0.2063, Loss G: 2.7793\n",
      "Epoch [16/40] Batch 600/2324 Loss D: 0.2128, Loss G: 2.8248\n",
      "Epoch [16/40] Batch 700/2324 Loss D: 0.2728, Loss G: 2.6724\n",
      "Epoch [16/40] Batch 800/2324 Loss D: 0.2059, Loss G: 2.7259\n",
      "Epoch [16/40] Batch 900/2324 Loss D: 0.2029, Loss G: 2.6167\n",
      "Epoch [16/40] Batch 1000/2324 Loss D: 0.2502, Loss G: 2.5796\n",
      "Epoch [16/40] Batch 1100/2324 Loss D: 0.1918, Loss G: 2.7515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/40] Batch 1200/2324 Loss D: 0.2118, Loss G: 2.7100\n",
      "Epoch [16/40] Batch 1300/2324 Loss D: 0.2348, Loss G: 2.7376\n",
      "Epoch [16/40] Batch 1400/2324 Loss D: 0.1683, Loss G: 2.7535\n",
      "Epoch [16/40] Batch 1500/2324 Loss D: 0.2497, Loss G: 2.5186\n",
      "Epoch [16/40] Batch 1600/2324 Loss D: 0.1921, Loss G: 2.9117\n",
      "Epoch [16/40] Batch 1700/2324 Loss D: 0.2506, Loss G: 2.6588\n",
      "Epoch [16/40] Batch 1800/2324 Loss D: 0.1890, Loss G: 2.7602\n",
      "Epoch [16/40] Batch 1900/2324 Loss D: 0.2569, Loss G: 2.5326\n",
      "Epoch [16/40] Batch 2000/2324 Loss D: 0.2195, Loss G: 2.7828\n",
      "Epoch [16/40] Batch 2100/2324 Loss D: 0.2380, Loss G: 2.6159\n",
      "Epoch [16/40] Batch 2200/2324 Loss D: 0.2280, Loss G: 2.7130\n",
      "Epoch [16/40] Batch 2300/2324 Loss D: 0.1660, Loss G: 3.1002\n",
      "Saved generator state dict to generator.pth after epoch 17\n",
      "Epoch [17/40] Batch 0/2324 Loss D: 0.2502, Loss G: 2.7926\n",
      "Epoch [17/40] Batch 100/2324 Loss D: 0.1670, Loss G: 2.8358\n",
      "Epoch [17/40] Batch 200/2324 Loss D: 0.1791, Loss G: 2.8888\n",
      "Epoch [17/40] Batch 300/2324 Loss D: 0.2562, Loss G: 2.6458\n",
      "Epoch [17/40] Batch 400/2324 Loss D: 0.2525, Loss G: 2.6277\n",
      "Epoch [17/40] Batch 500/2324 Loss D: 0.2021, Loss G: 2.6135\n",
      "Epoch [17/40] Batch 600/2324 Loss D: 0.2914, Loss G: 2.6462\n",
      "Epoch [17/40] Batch 700/2324 Loss D: 0.2513, Loss G: 2.5927\n",
      "Epoch [17/40] Batch 800/2324 Loss D: 0.2719, Loss G: 2.7290\n",
      "Epoch [17/40] Batch 900/2324 Loss D: 0.2696, Loss G: 2.6210\n",
      "Epoch [17/40] Batch 1000/2324 Loss D: 0.1398, Loss G: 2.8186\n",
      "Epoch [17/40] Batch 1100/2324 Loss D: 0.2382, Loss G: 2.8448\n",
      "Epoch [17/40] Batch 1200/2324 Loss D: 0.2476, Loss G: 2.6529\n",
      "Epoch [17/40] Batch 1300/2324 Loss D: 0.2084, Loss G: 2.8184\n",
      "Epoch [17/40] Batch 1400/2324 Loss D: 0.2566, Loss G: 2.7560\n",
      "Epoch [17/40] Batch 1500/2324 Loss D: 0.2045, Loss G: 2.9696\n",
      "Epoch [17/40] Batch 1600/2324 Loss D: 0.2519, Loss G: 2.6306\n",
      "Epoch [17/40] Batch 1700/2324 Loss D: 0.1915, Loss G: 3.0753\n",
      "Epoch [17/40] Batch 1800/2324 Loss D: 0.3107, Loss G: 2.4546\n",
      "Epoch [17/40] Batch 1900/2324 Loss D: 0.2548, Loss G: 2.6315\n",
      "Epoch [17/40] Batch 2000/2324 Loss D: 0.2395, Loss G: 3.0025\n",
      "Epoch [17/40] Batch 2100/2324 Loss D: 0.2537, Loss G: 2.8108\n",
      "Epoch [17/40] Batch 2200/2324 Loss D: 0.2358, Loss G: 2.6601\n",
      "Epoch [17/40] Batch 2300/2324 Loss D: 0.2381, Loss G: 2.7356\n",
      "Saved generator state dict to generator.pth after epoch 18\n",
      "Epoch [18/40] Batch 0/2324 Loss D: 0.3183, Loss G: 2.3195\n",
      "Epoch [18/40] Batch 100/2324 Loss D: 0.2325, Loss G: 2.8787\n",
      "Epoch [18/40] Batch 200/2324 Loss D: 0.2235, Loss G: 2.7509\n",
      "Epoch [18/40] Batch 300/2324 Loss D: 0.3009, Loss G: 2.6177\n",
      "Epoch [18/40] Batch 400/2324 Loss D: 0.3194, Loss G: 2.5865\n",
      "Epoch [18/40] Batch 500/2324 Loss D: 0.2375, Loss G: 2.8934\n",
      "Epoch [18/40] Batch 600/2324 Loss D: 0.2039, Loss G: 2.8398\n",
      "Epoch [18/40] Batch 700/2324 Loss D: 0.2550, Loss G: 2.5833\n",
      "Epoch [18/40] Batch 800/2324 Loss D: 0.2344, Loss G: 2.5980\n",
      "Epoch [18/40] Batch 900/2324 Loss D: 0.2542, Loss G: 2.9707\n",
      "Epoch [18/40] Batch 1000/2324 Loss D: 0.3051, Loss G: 2.7407\n",
      "Epoch [18/40] Batch 1100/2324 Loss D: 0.3164, Loss G: 2.2143\n",
      "Epoch [18/40] Batch 1200/2324 Loss D: 0.2840, Loss G: 2.3164\n",
      "Epoch [18/40] Batch 1300/2324 Loss D: 0.2731, Loss G: 2.2683\n",
      "Epoch [18/40] Batch 1400/2324 Loss D: 0.2633, Loss G: 2.2006\n",
      "Epoch [18/40] Batch 1500/2324 Loss D: 0.2964, Loss G: 2.7058\n",
      "Epoch [18/40] Batch 1600/2324 Loss D: 0.2824, Loss G: 2.7796\n",
      "Epoch [18/40] Batch 1700/2324 Loss D: 0.3124, Loss G: 2.5033\n",
      "Epoch [18/40] Batch 1800/2324 Loss D: 0.3127, Loss G: 2.3037\n",
      "Epoch [18/40] Batch 1900/2324 Loss D: 0.2943, Loss G: 2.1788\n",
      "Epoch [18/40] Batch 2000/2324 Loss D: 0.2612, Loss G: 2.2112\n",
      "Epoch [18/40] Batch 2100/2324 Loss D: 0.2227, Loss G: 2.4877\n",
      "Epoch [18/40] Batch 2200/2324 Loss D: 0.2697, Loss G: 2.5125\n",
      "Epoch [18/40] Batch 2300/2324 Loss D: 0.2690, Loss G: 2.4286\n",
      "Saved generator state dict to generator.pth after epoch 19\n",
      "Epoch [19/40] Batch 0/2324 Loss D: 0.3251, Loss G: 2.4779\n",
      "Epoch [19/40] Batch 100/2324 Loss D: 0.3368, Loss G: 2.2919\n",
      "Epoch [19/40] Batch 200/2324 Loss D: 0.2575, Loss G: 2.6439\n",
      "Epoch [19/40] Batch 300/2324 Loss D: 0.3337, Loss G: 2.3282\n",
      "Epoch [19/40] Batch 400/2324 Loss D: 0.2799, Loss G: 2.5245\n",
      "Epoch [19/40] Batch 500/2324 Loss D: 0.2809, Loss G: 2.5553\n",
      "Epoch [19/40] Batch 600/2324 Loss D: 0.2653, Loss G: 2.7138\n",
      "Epoch [19/40] Batch 700/2324 Loss D: 0.2270, Loss G: 2.7675\n",
      "Epoch [19/40] Batch 800/2324 Loss D: 0.2245, Loss G: 2.7538\n",
      "Epoch [19/40] Batch 900/2324 Loss D: 0.3438, Loss G: 2.4809\n",
      "Epoch [19/40] Batch 1000/2324 Loss D: 0.2346, Loss G: 2.6694\n",
      "Epoch [19/40] Batch 1100/2324 Loss D: 0.2267, Loss G: 2.8094\n",
      "Epoch [19/40] Batch 1200/2324 Loss D: 0.2969, Loss G: 2.6385\n",
      "Epoch [19/40] Batch 1300/2324 Loss D: 0.3050, Loss G: 2.6433\n",
      "Epoch [19/40] Batch 1400/2324 Loss D: 0.2877, Loss G: 2.6443\n",
      "Epoch [19/40] Batch 1500/2324 Loss D: 0.3280, Loss G: 2.8493\n",
      "Epoch [19/40] Batch 1600/2324 Loss D: 0.2765, Loss G: 2.5614\n",
      "Epoch [19/40] Batch 1700/2324 Loss D: 0.3123, Loss G: 2.3234\n",
      "Epoch [19/40] Batch 1800/2324 Loss D: 0.3800, Loss G: 2.3822\n",
      "Epoch [19/40] Batch 1900/2324 Loss D: 0.2319, Loss G: 2.6369\n",
      "Epoch [19/40] Batch 2000/2324 Loss D: 0.2716, Loss G: 2.5128\n",
      "Epoch [19/40] Batch 2100/2324 Loss D: 0.3357, Loss G: 2.4500\n",
      "Epoch [19/40] Batch 2200/2324 Loss D: 0.3242, Loss G: 2.5681\n",
      "Epoch [19/40] Batch 2300/2324 Loss D: 0.2163, Loss G: 2.6735\n",
      "Saved generator state dict to generator.pth after epoch 20\n",
      "Epoch [20/40] Batch 0/2324 Loss D: 0.2731, Loss G: 2.7299\n",
      "Epoch [20/40] Batch 100/2324 Loss D: 0.2753, Loss G: 2.4689\n",
      "Epoch [20/40] Batch 200/2324 Loss D: 0.2723, Loss G: 2.3092\n",
      "Epoch [20/40] Batch 300/2324 Loss D: 0.3194, Loss G: 2.3705\n",
      "Epoch [20/40] Batch 400/2324 Loss D: 0.2578, Loss G: 2.6799\n",
      "Epoch [20/40] Batch 500/2324 Loss D: 0.3736, Loss G: 2.2667\n",
      "Epoch [20/40] Batch 600/2324 Loss D: 0.2732, Loss G: 2.5543\n",
      "Epoch [20/40] Batch 700/2324 Loss D: 0.3219, Loss G: 2.0850\n",
      "Epoch [20/40] Batch 800/2324 Loss D: 0.3140, Loss G: 2.4799\n",
      "Epoch [20/40] Batch 900/2324 Loss D: 0.2445, Loss G: 2.5458\n",
      "Epoch [20/40] Batch 1000/2324 Loss D: 0.3445, Loss G: 2.2920\n",
      "Epoch [20/40] Batch 1100/2324 Loss D: 0.3501, Loss G: 2.1096\n",
      "Epoch [20/40] Batch 1200/2324 Loss D: 0.3282, Loss G: 2.4138\n",
      "Epoch [20/40] Batch 1300/2324 Loss D: 0.2927, Loss G: 2.1941\n",
      "Epoch [20/40] Batch 1400/2324 Loss D: 0.3671, Loss G: 2.2249\n",
      "Epoch [20/40] Batch 1500/2324 Loss D: 0.2464, Loss G: 2.5719\n",
      "Epoch [20/40] Batch 1600/2324 Loss D: 0.2741, Loss G: 2.3664\n",
      "Epoch [20/40] Batch 1700/2324 Loss D: 0.3237, Loss G: 2.4486\n",
      "Epoch [20/40] Batch 1800/2324 Loss D: 0.2585, Loss G: 2.7212\n",
      "Epoch [20/40] Batch 1900/2324 Loss D: 0.2087, Loss G: 2.6186\n",
      "Epoch [20/40] Batch 2000/2324 Loss D: 0.2696, Loss G: 2.6608\n",
      "Epoch [20/40] Batch 2100/2324 Loss D: 0.2794, Loss G: 2.7409\n",
      "Epoch [20/40] Batch 2200/2324 Loss D: 0.2921, Loss G: 2.3536\n",
      "Epoch [20/40] Batch 2300/2324 Loss D: 0.3334, Loss G: 2.2038\n",
      "Saved generator state dict to generator.pth after epoch 21\n",
      "Epoch [21/40] Batch 0/2324 Loss D: 0.2555, Loss G: 2.5706\n",
      "Epoch [21/40] Batch 100/2324 Loss D: 0.3460, Loss G: 2.2999\n",
      "Epoch [21/40] Batch 200/2324 Loss D: 0.2956, Loss G: 2.7505\n",
      "Epoch [21/40] Batch 300/2324 Loss D: 0.3082, Loss G: 2.5486\n",
      "Epoch [21/40] Batch 400/2324 Loss D: 0.2594, Loss G: 2.6387\n",
      "Epoch [21/40] Batch 500/2324 Loss D: 0.3294, Loss G: 2.4479\n",
      "Epoch [21/40] Batch 600/2324 Loss D: 0.2730, Loss G: 2.4885\n",
      "Epoch [21/40] Batch 700/2324 Loss D: 0.2287, Loss G: 2.4214\n",
      "Epoch [21/40] Batch 800/2324 Loss D: 0.3084, Loss G: 2.4803\n",
      "Epoch [21/40] Batch 900/2324 Loss D: 0.2959, Loss G: 2.7421\n",
      "Epoch [21/40] Batch 1000/2324 Loss D: 0.3193, Loss G: 2.7299\n",
      "Epoch [21/40] Batch 1100/2324 Loss D: 0.3005, Loss G: 2.3027\n",
      "Epoch [21/40] Batch 1200/2324 Loss D: 0.3142, Loss G: 2.2634\n",
      "Epoch [21/40] Batch 1300/2324 Loss D: 0.3073, Loss G: 2.5204\n",
      "Epoch [21/40] Batch 1400/2324 Loss D: 0.3109, Loss G: 2.4686\n",
      "Epoch [21/40] Batch 1500/2324 Loss D: 0.2608, Loss G: 2.4199\n",
      "Epoch [21/40] Batch 1600/2324 Loss D: 0.3279, Loss G: 2.4799\n",
      "Epoch [21/40] Batch 1700/2324 Loss D: 0.3571, Loss G: 2.6480\n",
      "Epoch [21/40] Batch 1800/2324 Loss D: 0.2623, Loss G: 2.4495\n",
      "Epoch [21/40] Batch 1900/2324 Loss D: 0.3152, Loss G: 2.6032\n",
      "Epoch [21/40] Batch 2000/2324 Loss D: 0.3290, Loss G: 2.1875\n",
      "Epoch [21/40] Batch 2100/2324 Loss D: 0.2980, Loss G: 2.5408\n",
      "Epoch [21/40] Batch 2200/2324 Loss D: 0.3545, Loss G: 2.4292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/40] Batch 2300/2324 Loss D: 0.3195, Loss G: 2.4279\n",
      "Saved generator state dict to generator.pth after epoch 22\n",
      "Epoch [22/40] Batch 0/2324 Loss D: 0.3415, Loss G: 2.4175\n",
      "Epoch [22/40] Batch 100/2324 Loss D: 0.2998, Loss G: 2.4844\n",
      "Epoch [22/40] Batch 200/2324 Loss D: 0.4380, Loss G: 2.0715\n",
      "Epoch [22/40] Batch 300/2324 Loss D: 0.3101, Loss G: 2.8285\n",
      "Epoch [22/40] Batch 400/2324 Loss D: 0.2214, Loss G: 2.3160\n",
      "Epoch [22/40] Batch 500/2324 Loss D: 0.3130, Loss G: 2.4802\n",
      "Epoch [22/40] Batch 600/2324 Loss D: 0.3760, Loss G: 1.9517\n",
      "Epoch [22/40] Batch 700/2324 Loss D: 0.2422, Loss G: 2.2931\n",
      "Epoch [22/40] Batch 800/2324 Loss D: 0.3104, Loss G: 2.4668\n",
      "Epoch [22/40] Batch 900/2324 Loss D: 0.2920, Loss G: 2.6549\n",
      "Epoch [22/40] Batch 1000/2324 Loss D: 0.3508, Loss G: 2.2363\n",
      "Epoch [22/40] Batch 1100/2324 Loss D: 0.2945, Loss G: 2.6584\n",
      "Epoch [22/40] Batch 1200/2324 Loss D: 0.3650, Loss G: 2.2812\n",
      "Epoch [22/40] Batch 1300/2324 Loss D: 0.3288, Loss G: 2.1397\n",
      "Epoch [22/40] Batch 1400/2324 Loss D: 0.3233, Loss G: 2.4239\n",
      "Epoch [22/40] Batch 1500/2324 Loss D: 0.3487, Loss G: 2.2027\n",
      "Epoch [22/40] Batch 1600/2324 Loss D: 0.2886, Loss G: 2.1567\n",
      "Epoch [22/40] Batch 1700/2324 Loss D: 0.3261, Loss G: 2.7353\n",
      "Epoch [22/40] Batch 1800/2324 Loss D: 0.3167, Loss G: 2.3541\n",
      "Epoch [22/40] Batch 1900/2324 Loss D: 0.3711, Loss G: 2.4503\n",
      "Epoch [22/40] Batch 2000/2324 Loss D: 0.2427, Loss G: 2.4917\n",
      "Epoch [22/40] Batch 2100/2324 Loss D: 0.2560, Loss G: 2.4224\n",
      "Epoch [22/40] Batch 2200/2324 Loss D: 0.4019, Loss G: 2.0138\n",
      "Epoch [22/40] Batch 2300/2324 Loss D: 0.2911, Loss G: 2.4486\n",
      "Saved generator state dict to generator.pth after epoch 23\n",
      "Epoch [23/40] Batch 0/2324 Loss D: 0.3272, Loss G: 2.1679\n",
      "Epoch [23/40] Batch 100/2324 Loss D: 0.3184, Loss G: 2.2295\n",
      "Epoch [23/40] Batch 200/2324 Loss D: 0.3187, Loss G: 2.1620\n",
      "Epoch [23/40] Batch 300/2324 Loss D: 0.3170, Loss G: 2.3874\n",
      "Epoch [23/40] Batch 400/2324 Loss D: 0.3556, Loss G: 2.1385\n",
      "Epoch [23/40] Batch 500/2324 Loss D: 0.3367, Loss G: 2.3007\n",
      "Epoch [23/40] Batch 600/2324 Loss D: 0.3115, Loss G: 2.5975\n",
      "Epoch [23/40] Batch 700/2324 Loss D: 0.3631, Loss G: 2.3696\n",
      "Epoch [23/40] Batch 800/2324 Loss D: 0.3558, Loss G: 2.2235\n",
      "Epoch [23/40] Batch 900/2324 Loss D: 0.2891, Loss G: 2.6886\n",
      "Epoch [23/40] Batch 1000/2324 Loss D: 0.2702, Loss G: 2.5323\n",
      "Epoch [23/40] Batch 1100/2324 Loss D: 0.4290, Loss G: 2.4074\n",
      "Epoch [23/40] Batch 1200/2324 Loss D: 0.3074, Loss G: 2.3787\n",
      "Epoch [23/40] Batch 1300/2324 Loss D: 0.2731, Loss G: 2.5291\n",
      "Epoch [23/40] Batch 1400/2324 Loss D: 0.3138, Loss G: 2.3948\n",
      "Epoch [23/40] Batch 1500/2324 Loss D: 0.3032, Loss G: 2.4536\n",
      "Epoch [23/40] Batch 1600/2324 Loss D: 0.3114, Loss G: 2.9001\n",
      "Epoch [23/40] Batch 1700/2324 Loss D: 0.3394, Loss G: 2.5322\n",
      "Epoch [23/40] Batch 1800/2324 Loss D: 0.3398, Loss G: 2.3843\n",
      "Epoch [23/40] Batch 1900/2324 Loss D: 0.3014, Loss G: 2.5180\n",
      "Epoch [23/40] Batch 2000/2324 Loss D: 0.3779, Loss G: 2.5631\n",
      "Epoch [23/40] Batch 2100/2324 Loss D: 0.2685, Loss G: 2.4715\n",
      "Epoch [23/40] Batch 2200/2324 Loss D: 0.3790, Loss G: 2.1962\n",
      "Epoch [23/40] Batch 2300/2324 Loss D: 0.4315, Loss G: 1.9368\n",
      "Saved generator state dict to generator.pth after epoch 24\n",
      "Epoch [24/40] Batch 0/2324 Loss D: 0.3249, Loss G: 2.1993\n",
      "Epoch [24/40] Batch 100/2324 Loss D: 0.3221, Loss G: 2.2383\n",
      "Epoch [24/40] Batch 200/2324 Loss D: 0.3048, Loss G: 2.2124\n",
      "Epoch [24/40] Batch 300/2324 Loss D: 0.3675, Loss G: 2.3055\n",
      "Epoch [24/40] Batch 400/2324 Loss D: 0.3305, Loss G: 2.1718\n",
      "Epoch [24/40] Batch 500/2324 Loss D: 0.3797, Loss G: 2.3162\n",
      "Epoch [24/40] Batch 600/2324 Loss D: 0.3546, Loss G: 2.4882\n",
      "Epoch [24/40] Batch 700/2324 Loss D: 0.3124, Loss G: 2.2987\n",
      "Epoch [24/40] Batch 800/2324 Loss D: 0.3792, Loss G: 2.3452\n",
      "Epoch [24/40] Batch 900/2324 Loss D: 0.2780, Loss G: 2.7611\n",
      "Epoch [24/40] Batch 1000/2324 Loss D: 0.3425, Loss G: 2.3606\n",
      "Epoch [24/40] Batch 1100/2324 Loss D: 0.2948, Loss G: 2.4393\n",
      "Epoch [24/40] Batch 1200/2324 Loss D: 0.3076, Loss G: 2.4728\n",
      "Epoch [24/40] Batch 1300/2324 Loss D: 0.3343, Loss G: 2.5843\n",
      "Epoch [24/40] Batch 1400/2324 Loss D: 0.3200, Loss G: 2.4063\n",
      "Epoch [24/40] Batch 1500/2324 Loss D: 0.3326, Loss G: 2.4231\n",
      "Epoch [24/40] Batch 1600/2324 Loss D: 0.2926, Loss G: 2.6411\n",
      "Epoch [24/40] Batch 1700/2324 Loss D: 0.2726, Loss G: 2.7304\n",
      "Epoch [24/40] Batch 1800/2324 Loss D: 0.3045, Loss G: 2.4772\n",
      "Epoch [24/40] Batch 1900/2324 Loss D: 0.4055, Loss G: 2.1819\n",
      "Epoch [24/40] Batch 2000/2324 Loss D: 0.3167, Loss G: 2.4950\n",
      "Epoch [24/40] Batch 2100/2324 Loss D: 0.3503, Loss G: 2.3308\n",
      "Epoch [24/40] Batch 2200/2324 Loss D: 0.3178, Loss G: 2.1864\n",
      "Epoch [24/40] Batch 2300/2324 Loss D: 0.3519, Loss G: 2.2368\n",
      "Saved generator state dict to generator.pth after epoch 25\n",
      "Epoch [25/40] Batch 0/2324 Loss D: 0.3633, Loss G: 2.4608\n",
      "Epoch [25/40] Batch 100/2324 Loss D: 0.2845, Loss G: 2.7592\n",
      "Epoch [25/40] Batch 200/2324 Loss D: 0.3343, Loss G: 2.4289\n",
      "Epoch [25/40] Batch 300/2324 Loss D: 0.3864, Loss G: 2.4350\n",
      "Epoch [25/40] Batch 400/2324 Loss D: 0.4425, Loss G: 2.3164\n",
      "Epoch [25/40] Batch 500/2324 Loss D: 0.3331, Loss G: 2.1797\n",
      "Epoch [25/40] Batch 600/2324 Loss D: 0.2100, Loss G: 2.6249\n",
      "Epoch [25/40] Batch 700/2324 Loss D: 0.3257, Loss G: 2.4985\n",
      "Epoch [25/40] Batch 800/2324 Loss D: 0.3655, Loss G: 2.2743\n",
      "Epoch [25/40] Batch 900/2324 Loss D: 0.3068, Loss G: 2.3257\n",
      "Epoch [25/40] Batch 1000/2324 Loss D: 0.3587, Loss G: 2.5398\n",
      "Epoch [25/40] Batch 1100/2324 Loss D: 0.2826, Loss G: 2.5941\n",
      "Epoch [25/40] Batch 1200/2324 Loss D: 0.2606, Loss G: 2.7038\n",
      "Epoch [25/40] Batch 1300/2324 Loss D: 0.2510, Loss G: 2.9593\n",
      "Epoch [25/40] Batch 1400/2324 Loss D: 0.3885, Loss G: 2.5741\n",
      "Epoch [25/40] Batch 1500/2324 Loss D: 0.3082, Loss G: 2.1995\n",
      "Epoch [25/40] Batch 1600/2324 Loss D: 0.2648, Loss G: 2.6681\n",
      "Epoch [25/40] Batch 1700/2324 Loss D: 0.3200, Loss G: 2.1370\n",
      "Epoch [25/40] Batch 1800/2324 Loss D: 0.3648, Loss G: 2.4621\n",
      "Epoch [25/40] Batch 1900/2324 Loss D: 0.3514, Loss G: 2.4337\n",
      "Epoch [25/40] Batch 2000/2324 Loss D: 0.3448, Loss G: 2.3488\n",
      "Epoch [25/40] Batch 2100/2324 Loss D: 0.2477, Loss G: 2.4047\n",
      "Epoch [25/40] Batch 2200/2324 Loss D: 0.2833, Loss G: 2.3774\n",
      "Epoch [25/40] Batch 2300/2324 Loss D: 0.3150, Loss G: 2.5696\n",
      "Saved generator state dict to generator.pth after epoch 26\n",
      "Epoch [26/40] Batch 0/2324 Loss D: 0.3596, Loss G: 2.2389\n",
      "Epoch [26/40] Batch 100/2324 Loss D: 0.2839, Loss G: 2.4842\n",
      "Epoch [26/40] Batch 200/2324 Loss D: 0.2621, Loss G: 2.5261\n",
      "Epoch [26/40] Batch 300/2324 Loss D: 0.3591, Loss G: 2.5782\n",
      "Epoch [26/40] Batch 400/2324 Loss D: 0.2912, Loss G: 2.8248\n",
      "Epoch [26/40] Batch 500/2324 Loss D: 0.2483, Loss G: 2.4661\n",
      "Epoch [26/40] Batch 600/2324 Loss D: 0.2402, Loss G: 3.1546\n",
      "Epoch [26/40] Batch 700/2324 Loss D: 0.3468, Loss G: 2.4891\n",
      "Epoch [26/40] Batch 800/2324 Loss D: 0.3620, Loss G: 2.3864\n",
      "Epoch [26/40] Batch 900/2324 Loss D: 0.3451, Loss G: 2.2749\n",
      "Epoch [26/40] Batch 1000/2324 Loss D: 0.3047, Loss G: 2.4014\n",
      "Epoch [26/40] Batch 1100/2324 Loss D: 0.3062, Loss G: 2.5285\n",
      "Epoch [26/40] Batch 1200/2324 Loss D: 0.2649, Loss G: 2.3366\n",
      "Epoch [26/40] Batch 1300/2324 Loss D: 0.2539, Loss G: 2.4714\n",
      "Epoch [26/40] Batch 1400/2324 Loss D: 0.3349, Loss G: 2.1827\n",
      "Epoch [26/40] Batch 1500/2324 Loss D: 0.3692, Loss G: 2.1838\n",
      "Epoch [26/40] Batch 1600/2324 Loss D: 0.3684, Loss G: 2.4382\n",
      "Epoch [26/40] Batch 1700/2324 Loss D: 0.3877, Loss G: 2.0944\n",
      "Epoch [26/40] Batch 1800/2324 Loss D: 0.3863, Loss G: 2.0702\n",
      "Epoch [26/40] Batch 1900/2324 Loss D: 0.2829, Loss G: 2.3795\n",
      "Epoch [26/40] Batch 2000/2324 Loss D: 0.2553, Loss G: 2.3824\n",
      "Epoch [26/40] Batch 2100/2324 Loss D: 0.3829, Loss G: 2.2279\n",
      "Epoch [26/40] Batch 2200/2324 Loss D: 0.3751, Loss G: 2.2588\n",
      "Epoch [26/40] Batch 2300/2324 Loss D: 0.4232, Loss G: 2.0594\n",
      "Saved generator state dict to generator.pth after epoch 27\n",
      "Epoch [27/40] Batch 0/2324 Loss D: 0.3631, Loss G: 2.3224\n",
      "Epoch [27/40] Batch 100/2324 Loss D: 0.3207, Loss G: 2.4861\n",
      "Epoch [27/40] Batch 200/2324 Loss D: 0.3538, Loss G: 1.9518\n",
      "Epoch [27/40] Batch 300/2324 Loss D: 0.2415, Loss G: 2.4910\n",
      "Epoch [27/40] Batch 400/2324 Loss D: 0.4027, Loss G: 2.1468\n",
      "Epoch [27/40] Batch 500/2324 Loss D: 0.3309, Loss G: 2.3556\n",
      "Epoch [27/40] Batch 600/2324 Loss D: 0.3114, Loss G: 2.6932\n",
      "Epoch [27/40] Batch 700/2324 Loss D: 0.3131, Loss G: 2.5501\n",
      "Epoch [27/40] Batch 800/2324 Loss D: 0.2758, Loss G: 2.3700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/40] Batch 900/2324 Loss D: 0.2981, Loss G: 2.4216\n",
      "Epoch [27/40] Batch 1000/2324 Loss D: 0.2836, Loss G: 2.2175\n",
      "Epoch [27/40] Batch 1100/2324 Loss D: 0.3264, Loss G: 2.3996\n",
      "Epoch [27/40] Batch 1200/2324 Loss D: 0.3129, Loss G: 2.6133\n",
      "Epoch [27/40] Batch 1300/2324 Loss D: 0.3596, Loss G: 2.1902\n",
      "Epoch [27/40] Batch 1400/2324 Loss D: 0.3158, Loss G: 2.5396\n",
      "Epoch [27/40] Batch 1500/2324 Loss D: 0.3423, Loss G: 2.6584\n",
      "Epoch [27/40] Batch 1600/2324 Loss D: 0.3474, Loss G: 2.3160\n",
      "Epoch [27/40] Batch 1700/2324 Loss D: 0.4266, Loss G: 2.4899\n",
      "Epoch [27/40] Batch 1800/2324 Loss D: 0.3132, Loss G: 2.7099\n",
      "Epoch [27/40] Batch 1900/2324 Loss D: 0.2453, Loss G: 2.7778\n",
      "Epoch [27/40] Batch 2000/2324 Loss D: 0.3486, Loss G: 2.5883\n",
      "Epoch [27/40] Batch 2100/2324 Loss D: 0.3870, Loss G: 2.5776\n",
      "Epoch [27/40] Batch 2200/2324 Loss D: 0.2576, Loss G: 2.8495\n",
      "Epoch [27/40] Batch 2300/2324 Loss D: 0.2637, Loss G: 2.4744\n",
      "Saved generator state dict to generator.pth after epoch 28\n",
      "Epoch [28/40] Batch 0/2324 Loss D: 0.3301, Loss G: 2.5422\n",
      "Epoch [28/40] Batch 100/2324 Loss D: 0.3089, Loss G: 2.5272\n",
      "Epoch [28/40] Batch 200/2324 Loss D: 0.3408, Loss G: 2.0126\n",
      "Epoch [28/40] Batch 300/2324 Loss D: 0.3739, Loss G: 2.1084\n",
      "Epoch [28/40] Batch 400/2324 Loss D: 0.3161, Loss G: 2.1736\n",
      "Epoch [28/40] Batch 500/2324 Loss D: 0.4012, Loss G: 2.1807\n",
      "Epoch [28/40] Batch 600/2324 Loss D: 0.3369, Loss G: 2.2876\n",
      "Epoch [28/40] Batch 700/2324 Loss D: 0.4490, Loss G: 2.3286\n",
      "Epoch [28/40] Batch 800/2324 Loss D: 0.3685, Loss G: 2.4499\n",
      "Epoch [28/40] Batch 900/2324 Loss D: 0.3984, Loss G: 2.5266\n",
      "Epoch [28/40] Batch 1000/2324 Loss D: 0.3463, Loss G: 2.4419\n",
      "Epoch [28/40] Batch 1100/2324 Loss D: 0.4021, Loss G: 2.2063\n",
      "Epoch [28/40] Batch 1200/2324 Loss D: 0.3543, Loss G: 2.3795\n",
      "Epoch [28/40] Batch 1300/2324 Loss D: 0.3358, Loss G: 2.5270\n",
      "Epoch [28/40] Batch 1400/2324 Loss D: 0.3768, Loss G: 2.2883\n",
      "Epoch [28/40] Batch 1500/2324 Loss D: 0.2999, Loss G: 2.2039\n",
      "Epoch [28/40] Batch 1600/2324 Loss D: 0.3773, Loss G: 2.0480\n",
      "Epoch [28/40] Batch 1700/2324 Loss D: 0.3274, Loss G: 2.0989\n",
      "Epoch [28/40] Batch 1800/2324 Loss D: 0.3058, Loss G: 2.7414\n",
      "Epoch [28/40] Batch 1900/2324 Loss D: 0.2527, Loss G: 2.5766\n",
      "Epoch [28/40] Batch 2000/2324 Loss D: 0.2790, Loss G: 2.5933\n",
      "Epoch [28/40] Batch 2100/2324 Loss D: 0.3510, Loss G: 2.4181\n",
      "Epoch [28/40] Batch 2200/2324 Loss D: 0.3764, Loss G: 2.2878\n",
      "Epoch [28/40] Batch 2300/2324 Loss D: 0.2704, Loss G: 2.4175\n",
      "Saved generator state dict to generator.pth after epoch 29\n",
      "Epoch [29/40] Batch 0/2324 Loss D: 0.2931, Loss G: 2.4226\n",
      "Epoch [29/40] Batch 100/2324 Loss D: 0.2466, Loss G: 2.4662\n",
      "Epoch [29/40] Batch 200/2324 Loss D: 0.3272, Loss G: 2.5980\n",
      "Epoch [29/40] Batch 300/2324 Loss D: 0.3338, Loss G: 2.2026\n",
      "Epoch [29/40] Batch 400/2324 Loss D: 0.3875, Loss G: 2.1549\n",
      "Epoch [29/40] Batch 500/2324 Loss D: 0.4472, Loss G: 2.1774\n",
      "Epoch [29/40] Batch 600/2324 Loss D: 0.3270, Loss G: 2.5472\n",
      "Epoch [29/40] Batch 700/2324 Loss D: 0.3280, Loss G: 2.1655\n",
      "Epoch [29/40] Batch 800/2324 Loss D: 0.3063, Loss G: 2.2640\n",
      "Epoch [29/40] Batch 900/2324 Loss D: 0.4248, Loss G: 2.0850\n",
      "Epoch [29/40] Batch 1000/2324 Loss D: 0.2946, Loss G: 2.7426\n",
      "Epoch [29/40] Batch 1100/2324 Loss D: 0.3507, Loss G: 2.2620\n",
      "Epoch [29/40] Batch 1200/2324 Loss D: 0.3773, Loss G: 2.2162\n",
      "Epoch [29/40] Batch 1300/2324 Loss D: 0.3238, Loss G: 2.5229\n",
      "Epoch [29/40] Batch 1400/2324 Loss D: 0.2744, Loss G: 2.5442\n",
      "Epoch [29/40] Batch 1500/2324 Loss D: 0.3745, Loss G: 2.2747\n",
      "Epoch [29/40] Batch 1600/2324 Loss D: 0.2997, Loss G: 2.3943\n",
      "Epoch [29/40] Batch 1700/2324 Loss D: 0.3140, Loss G: 2.3947\n",
      "Epoch [29/40] Batch 1800/2324 Loss D: 0.3114, Loss G: 2.4673\n",
      "Epoch [29/40] Batch 1900/2324 Loss D: 0.3722, Loss G: 2.5090\n",
      "Epoch [29/40] Batch 2000/2324 Loss D: 0.3965, Loss G: 2.5692\n",
      "Epoch [29/40] Batch 2100/2324 Loss D: 0.2903, Loss G: 2.6416\n",
      "Epoch [29/40] Batch 2200/2324 Loss D: 0.4260, Loss G: 2.4019\n",
      "Epoch [29/40] Batch 2300/2324 Loss D: 0.3283, Loss G: 2.3119\n",
      "Saved generator state dict to generator.pth after epoch 30\n",
      "Epoch [30/40] Batch 0/2324 Loss D: 0.3549, Loss G: 2.6377\n",
      "Epoch [30/40] Batch 100/2324 Loss D: 0.2435, Loss G: 2.4442\n",
      "Epoch [30/40] Batch 200/2324 Loss D: 0.3984, Loss G: 2.0678\n",
      "Epoch [30/40] Batch 300/2324 Loss D: 0.3772, Loss G: 2.2975\n",
      "Epoch [30/40] Batch 400/2324 Loss D: 0.3348, Loss G: 2.3488\n",
      "Epoch [30/40] Batch 500/2324 Loss D: 0.2912, Loss G: 2.4928\n",
      "Epoch [30/40] Batch 600/2324 Loss D: 0.2815, Loss G: 2.5916\n",
      "Epoch [30/40] Batch 700/2324 Loss D: 0.3876, Loss G: 2.4731\n",
      "Epoch [30/40] Batch 800/2324 Loss D: 0.4210, Loss G: 2.3731\n",
      "Epoch [30/40] Batch 900/2324 Loss D: 0.4500, Loss G: 1.8994\n",
      "Epoch [30/40] Batch 1000/2324 Loss D: 0.3234, Loss G: 2.6339\n",
      "Epoch [30/40] Batch 1100/2324 Loss D: 0.3147, Loss G: 2.3587\n",
      "Epoch [30/40] Batch 1200/2324 Loss D: 0.2668, Loss G: 2.6934\n",
      "Epoch [30/40] Batch 1300/2324 Loss D: 0.2828, Loss G: 3.0673\n",
      "Epoch [30/40] Batch 1400/2324 Loss D: 0.3557, Loss G: 2.6941\n",
      "Epoch [30/40] Batch 1500/2324 Loss D: 0.3442, Loss G: 2.5882\n",
      "Epoch [30/40] Batch 1600/2324 Loss D: 0.3824, Loss G: 2.1507\n",
      "Epoch [30/40] Batch 1700/2324 Loss D: 0.3799, Loss G: 2.3239\n",
      "Epoch [30/40] Batch 1800/2324 Loss D: 0.3295, Loss G: 2.1983\n",
      "Epoch [30/40] Batch 1900/2324 Loss D: 0.3102, Loss G: 2.3638\n",
      "Epoch [30/40] Batch 2000/2324 Loss D: 0.3152, Loss G: 2.3796\n",
      "Epoch [30/40] Batch 2100/2324 Loss D: 0.3268, Loss G: 2.3694\n",
      "Epoch [30/40] Batch 2200/2324 Loss D: 0.3444, Loss G: 2.4280\n",
      "Epoch [30/40] Batch 2300/2324 Loss D: 0.4612, Loss G: 2.3288\n",
      "Saved generator state dict to generator.pth after epoch 31\n",
      "Epoch [31/40] Batch 0/2324 Loss D: 0.3835, Loss G: 2.1162\n",
      "Epoch [31/40] Batch 100/2324 Loss D: 0.3091, Loss G: 2.3449\n",
      "Epoch [31/40] Batch 200/2324 Loss D: 0.2214, Loss G: 2.8211\n",
      "Epoch [31/40] Batch 300/2324 Loss D: 0.3961, Loss G: 2.1983\n",
      "Epoch [31/40] Batch 400/2324 Loss D: 0.3922, Loss G: 2.2592\n",
      "Epoch [31/40] Batch 500/2324 Loss D: 0.4010, Loss G: 2.1020\n",
      "Epoch [31/40] Batch 600/2324 Loss D: 0.3564, Loss G: 2.2148\n",
      "Epoch [31/40] Batch 700/2324 Loss D: 0.2791, Loss G: 2.4407\n",
      "Epoch [31/40] Batch 800/2324 Loss D: 0.5008, Loss G: 2.0209\n",
      "Epoch [31/40] Batch 900/2324 Loss D: 0.3434, Loss G: 2.5151\n",
      "Epoch [31/40] Batch 1000/2324 Loss D: 0.3657, Loss G: 2.4067\n",
      "Epoch [31/40] Batch 1100/2324 Loss D: 0.2843, Loss G: 2.3535\n",
      "Epoch [31/40] Batch 1200/2324 Loss D: 0.3640, Loss G: 1.9141\n",
      "Epoch [31/40] Batch 1300/2324 Loss D: 0.3549, Loss G: 2.1413\n",
      "Epoch [31/40] Batch 1400/2324 Loss D: 0.3551, Loss G: 2.3216\n",
      "Epoch [31/40] Batch 1500/2324 Loss D: 0.3815, Loss G: 2.5273\n",
      "Epoch [31/40] Batch 1600/2324 Loss D: 0.3681, Loss G: 2.4428\n",
      "Epoch [31/40] Batch 1700/2324 Loss D: 0.4277, Loss G: 2.3216\n",
      "Epoch [31/40] Batch 1800/2324 Loss D: 0.3194, Loss G: 2.3193\n",
      "Epoch [31/40] Batch 1900/2324 Loss D: 0.3949, Loss G: 2.2407\n",
      "Epoch [31/40] Batch 2000/2324 Loss D: 0.2964, Loss G: 2.3876\n",
      "Epoch [31/40] Batch 2100/2324 Loss D: 0.3293, Loss G: 2.4195\n",
      "Epoch [31/40] Batch 2200/2324 Loss D: 0.3618, Loss G: 2.2543\n",
      "Epoch [31/40] Batch 2300/2324 Loss D: 0.3641, Loss G: 2.3294\n",
      "Saved generator state dict to generator.pth after epoch 32\n",
      "Epoch [32/40] Batch 0/2324 Loss D: 0.3385, Loss G: 2.5144\n",
      "Epoch [32/40] Batch 100/2324 Loss D: 0.3722, Loss G: 2.2448\n",
      "Epoch [32/40] Batch 200/2324 Loss D: 0.4383, Loss G: 2.4876\n",
      "Epoch [32/40] Batch 300/2324 Loss D: 0.3516, Loss G: 2.2162\n",
      "Epoch [32/40] Batch 400/2324 Loss D: 0.4014, Loss G: 2.2375\n",
      "Epoch [32/40] Batch 500/2324 Loss D: 0.3145, Loss G: 2.4771\n",
      "Epoch [32/40] Batch 600/2324 Loss D: 0.3042, Loss G: 2.2191\n",
      "Epoch [32/40] Batch 700/2324 Loss D: 0.4228, Loss G: 2.3330\n",
      "Epoch [32/40] Batch 800/2324 Loss D: 0.3540, Loss G: 2.5099\n",
      "Epoch [32/40] Batch 900/2324 Loss D: 0.3396, Loss G: 2.4831\n",
      "Epoch [32/40] Batch 1000/2324 Loss D: 0.3325, Loss G: 2.7015\n",
      "Epoch [32/40] Batch 1100/2324 Loss D: 0.3760, Loss G: 2.5463\n",
      "Epoch [32/40] Batch 1200/2324 Loss D: 0.2621, Loss G: 2.4333\n",
      "Epoch [32/40] Batch 1300/2324 Loss D: 0.2471, Loss G: 2.4218\n",
      "Epoch [32/40] Batch 1400/2324 Loss D: 0.3935, Loss G: 1.9922\n",
      "Epoch [32/40] Batch 1500/2324 Loss D: 0.4539, Loss G: 2.1795\n",
      "Epoch [32/40] Batch 1600/2324 Loss D: 0.3498, Loss G: 2.3953\n",
      "Epoch [32/40] Batch 1700/2324 Loss D: 0.3922, Loss G: 2.6363\n",
      "Epoch [32/40] Batch 1800/2324 Loss D: 0.3637, Loss G: 2.1364\n",
      "Epoch [32/40] Batch 1900/2324 Loss D: 0.3386, Loss G: 2.2977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/40] Batch 2000/2324 Loss D: 0.3913, Loss G: 2.1256\n",
      "Epoch [32/40] Batch 2100/2324 Loss D: 0.3275, Loss G: 2.5201\n",
      "Epoch [32/40] Batch 2200/2324 Loss D: 0.4213, Loss G: 1.9946\n",
      "Epoch [32/40] Batch 2300/2324 Loss D: 0.3744, Loss G: 2.4534\n",
      "Saved generator state dict to generator.pth after epoch 33\n",
      "Epoch [33/40] Batch 0/2324 Loss D: 0.2657, Loss G: 2.6732\n",
      "Epoch [33/40] Batch 100/2324 Loss D: 0.3166, Loss G: 2.8620\n",
      "Epoch [33/40] Batch 200/2324 Loss D: 0.3949, Loss G: 2.4136\n",
      "Epoch [33/40] Batch 300/2324 Loss D: 0.3346, Loss G: 2.4258\n",
      "Epoch [33/40] Batch 400/2324 Loss D: 0.3916, Loss G: 2.0667\n",
      "Epoch [33/40] Batch 500/2324 Loss D: 0.3343, Loss G: 2.4876\n",
      "Epoch [33/40] Batch 600/2324 Loss D: 0.2468, Loss G: 2.6740\n",
      "Epoch [33/40] Batch 700/2324 Loss D: 0.4137, Loss G: 2.4972\n",
      "Epoch [33/40] Batch 800/2324 Loss D: 0.4226, Loss G: 2.2591\n",
      "Epoch [33/40] Batch 900/2324 Loss D: 0.4031, Loss G: 2.3435\n",
      "Epoch [33/40] Batch 1000/2324 Loss D: 0.3201, Loss G: 2.4165\n",
      "Epoch [33/40] Batch 1100/2324 Loss D: 0.3505, Loss G: 2.4077\n",
      "Epoch [33/40] Batch 1200/2324 Loss D: 0.3610, Loss G: 2.2609\n",
      "Epoch [33/40] Batch 1300/2324 Loss D: 0.4192, Loss G: 2.2935\n",
      "Epoch [33/40] Batch 1400/2324 Loss D: 0.4612, Loss G: 2.4547\n",
      "Epoch [33/40] Batch 1500/2324 Loss D: 0.3408, Loss G: 2.4578\n",
      "Epoch [33/40] Batch 1600/2324 Loss D: 0.4128, Loss G: 2.2519\n",
      "Epoch [33/40] Batch 1700/2324 Loss D: 0.3044, Loss G: 2.3417\n",
      "Epoch [33/40] Batch 1800/2324 Loss D: 0.3086, Loss G: 2.4087\n",
      "Epoch [33/40] Batch 1900/2324 Loss D: 0.3235, Loss G: 2.2496\n",
      "Epoch [33/40] Batch 2000/2324 Loss D: 0.3810, Loss G: 2.4431\n",
      "Epoch [33/40] Batch 2100/2324 Loss D: 0.3038, Loss G: 2.7683\n",
      "Epoch [33/40] Batch 2200/2324 Loss D: 0.3119, Loss G: 2.1689\n",
      "Epoch [33/40] Batch 2300/2324 Loss D: 0.3207, Loss G: 2.5213\n",
      "Saved generator state dict to generator.pth after epoch 34\n",
      "Epoch [34/40] Batch 0/2324 Loss D: 0.2977, Loss G: 2.5845\n",
      "Epoch [34/40] Batch 100/2324 Loss D: 0.3326, Loss G: 2.4946\n",
      "Epoch [34/40] Batch 200/2324 Loss D: 0.3301, Loss G: 2.2318\n",
      "Epoch [34/40] Batch 300/2324 Loss D: 0.4038, Loss G: 2.6504\n",
      "Epoch [34/40] Batch 400/2324 Loss D: 0.3782, Loss G: 2.1156\n",
      "Epoch [34/40] Batch 500/2324 Loss D: 0.3158, Loss G: 2.3191\n",
      "Epoch [34/40] Batch 600/2324 Loss D: 0.3703, Loss G: 2.3074\n",
      "Epoch [34/40] Batch 700/2324 Loss D: 0.4327, Loss G: 2.2540\n",
      "Epoch [34/40] Batch 800/2324 Loss D: 0.3168, Loss G: 2.2748\n",
      "Epoch [34/40] Batch 900/2324 Loss D: 0.2948, Loss G: 2.3730\n",
      "Epoch [34/40] Batch 1000/2324 Loss D: 0.3298, Loss G: 2.1745\n",
      "Epoch [34/40] Batch 1100/2324 Loss D: 0.4622, Loss G: 2.0120\n",
      "Epoch [34/40] Batch 1200/2324 Loss D: 0.3919, Loss G: 2.2168\n",
      "Epoch [34/40] Batch 1300/2324 Loss D: 0.3015, Loss G: 2.1206\n",
      "Epoch [34/40] Batch 1400/2324 Loss D: 0.3307, Loss G: 2.2028\n",
      "Epoch [34/40] Batch 1500/2324 Loss D: 0.3616, Loss G: 2.3245\n",
      "Epoch [34/40] Batch 1600/2324 Loss D: 0.3264, Loss G: 2.2230\n",
      "Epoch [34/40] Batch 1700/2324 Loss D: 0.5558, Loss G: 2.0230\n",
      "Epoch [34/40] Batch 1800/2324 Loss D: 0.4092, Loss G: 2.4228\n",
      "Epoch [34/40] Batch 1900/2324 Loss D: 0.3871, Loss G: 2.4790\n",
      "Epoch [34/40] Batch 2000/2324 Loss D: 0.3280, Loss G: 2.4200\n",
      "Epoch [34/40] Batch 2100/2324 Loss D: 0.2819, Loss G: 2.3418\n",
      "Epoch [34/40] Batch 2200/2324 Loss D: 0.3048, Loss G: 2.5453\n",
      "Epoch [34/40] Batch 2300/2324 Loss D: 0.2897, Loss G: 2.3457\n",
      "Saved generator state dict to generator.pth after epoch 35\n",
      "Epoch [35/40] Batch 0/2324 Loss D: 0.3026, Loss G: 2.2880\n",
      "Epoch [35/40] Batch 100/2324 Loss D: 0.3806, Loss G: 2.0957\n",
      "Epoch [35/40] Batch 200/2324 Loss D: 0.3526, Loss G: 2.3792\n",
      "Epoch [35/40] Batch 300/2324 Loss D: 0.3528, Loss G: 2.1637\n",
      "Epoch [35/40] Batch 400/2324 Loss D: 0.3597, Loss G: 2.3705\n",
      "Epoch [35/40] Batch 500/2324 Loss D: 0.3949, Loss G: 2.3648\n",
      "Epoch [35/40] Batch 600/2324 Loss D: 0.3817, Loss G: 2.2601\n",
      "Epoch [35/40] Batch 700/2324 Loss D: 0.3496, Loss G: 2.4135\n",
      "Epoch [35/40] Batch 800/2324 Loss D: 0.3449, Loss G: 2.4398\n",
      "Epoch [35/40] Batch 900/2324 Loss D: 0.4025, Loss G: 2.3776\n",
      "Epoch [35/40] Batch 1000/2324 Loss D: 0.3287, Loss G: 2.4318\n",
      "Epoch [35/40] Batch 1100/2324 Loss D: 0.3346, Loss G: 2.3955\n",
      "Epoch [35/40] Batch 1200/2324 Loss D: 0.4301, Loss G: 2.2475\n",
      "Epoch [35/40] Batch 1300/2324 Loss D: 0.3658, Loss G: 2.1198\n",
      "Epoch [35/40] Batch 1400/2324 Loss D: 0.3700, Loss G: 2.4552\n",
      "Epoch [35/40] Batch 1500/2324 Loss D: 0.3740, Loss G: 2.5043\n",
      "Epoch [35/40] Batch 1600/2324 Loss D: 0.3778, Loss G: 2.2450\n",
      "Epoch [35/40] Batch 1700/2324 Loss D: 0.3426, Loss G: 2.3181\n",
      "Epoch [35/40] Batch 1800/2324 Loss D: 0.3578, Loss G: 2.1316\n",
      "Epoch [35/40] Batch 1900/2324 Loss D: 0.3893, Loss G: 2.5180\n",
      "Epoch [35/40] Batch 2000/2324 Loss D: 0.3323, Loss G: 2.2409\n",
      "Epoch [35/40] Batch 2100/2324 Loss D: 0.3292, Loss G: 2.2877\n",
      "Epoch [35/40] Batch 2200/2324 Loss D: 0.3902, Loss G: 1.8404\n",
      "Epoch [35/40] Batch 2300/2324 Loss D: 0.4821, Loss G: 2.1208\n",
      "Saved generator state dict to generator.pth after epoch 36\n",
      "Epoch [36/40] Batch 0/2324 Loss D: 0.4815, Loss G: 2.0683\n",
      "Epoch [36/40] Batch 100/2324 Loss D: 0.4315, Loss G: 2.4934\n",
      "Epoch [36/40] Batch 200/2324 Loss D: 0.3477, Loss G: 2.4117\n",
      "Epoch [36/40] Batch 300/2324 Loss D: 0.4072, Loss G: 2.0883\n",
      "Epoch [36/40] Batch 400/2324 Loss D: 0.4606, Loss G: 2.2272\n",
      "Epoch [36/40] Batch 500/2324 Loss D: 0.3483, Loss G: 2.1471\n",
      "Epoch [36/40] Batch 600/2324 Loss D: 0.3458, Loss G: 2.4145\n",
      "Epoch [36/40] Batch 700/2324 Loss D: 0.3745, Loss G: 2.3402\n",
      "Epoch [36/40] Batch 800/2324 Loss D: 0.4006, Loss G: 2.3643\n",
      "Epoch [36/40] Batch 900/2324 Loss D: 0.4403, Loss G: 2.2205\n",
      "Epoch [36/40] Batch 1000/2324 Loss D: 0.3687, Loss G: 2.1196\n",
      "Epoch [36/40] Batch 1100/2324 Loss D: 0.3219, Loss G: 2.5392\n",
      "Epoch [36/40] Batch 1200/2324 Loss D: 0.3975, Loss G: 2.3673\n",
      "Epoch [36/40] Batch 1300/2324 Loss D: 0.3658, Loss G: 2.2451\n",
      "Epoch [36/40] Batch 1400/2324 Loss D: 0.3582, Loss G: 2.4946\n",
      "Epoch [36/40] Batch 1500/2324 Loss D: 0.4751, Loss G: 2.4281\n",
      "Epoch [36/40] Batch 1600/2324 Loss D: 0.4095, Loss G: 2.0512\n",
      "Epoch [36/40] Batch 1700/2324 Loss D: 0.3839, Loss G: 2.1956\n",
      "Epoch [36/40] Batch 1800/2324 Loss D: 0.4257, Loss G: 2.4165\n",
      "Epoch [36/40] Batch 1900/2324 Loss D: 0.3476, Loss G: 2.1710\n",
      "Epoch [36/40] Batch 2000/2324 Loss D: 0.3571, Loss G: 2.2711\n",
      "Epoch [36/40] Batch 2100/2324 Loss D: 0.3697, Loss G: 2.4872\n",
      "Epoch [36/40] Batch 2200/2324 Loss D: 0.3162, Loss G: 2.2821\n",
      "Epoch [36/40] Batch 2300/2324 Loss D: 0.4708, Loss G: 1.9003\n",
      "Saved generator state dict to generator.pth after epoch 37\n",
      "Epoch [37/40] Batch 0/2324 Loss D: 0.3460, Loss G: 2.1808\n",
      "Epoch [37/40] Batch 100/2324 Loss D: 0.4144, Loss G: 2.2699\n",
      "Epoch [37/40] Batch 200/2324 Loss D: 0.4149, Loss G: 2.3775\n",
      "Epoch [37/40] Batch 300/2324 Loss D: 0.3667, Loss G: 2.0317\n",
      "Epoch [37/40] Batch 400/2324 Loss D: 0.3078, Loss G: 2.3552\n",
      "Epoch [37/40] Batch 500/2324 Loss D: 0.4091, Loss G: 2.6122\n",
      "Epoch [37/40] Batch 600/2324 Loss D: 0.4956, Loss G: 2.0193\n",
      "Epoch [37/40] Batch 700/2324 Loss D: 0.4207, Loss G: 2.1132\n",
      "Epoch [37/40] Batch 800/2324 Loss D: 0.3410, Loss G: 2.4021\n",
      "Epoch [37/40] Batch 900/2324 Loss D: 0.3241, Loss G: 2.1221\n",
      "Epoch [37/40] Batch 1000/2324 Loss D: 0.3533, Loss G: 2.1099\n",
      "Epoch [37/40] Batch 1100/2324 Loss D: 0.4554, Loss G: 1.9819\n",
      "Epoch [37/40] Batch 1200/2324 Loss D: 0.4729, Loss G: 2.1644\n",
      "Epoch [37/40] Batch 1300/2324 Loss D: 0.3526, Loss G: 2.3991\n",
      "Epoch [37/40] Batch 1400/2324 Loss D: 0.4531, Loss G: 2.5258\n",
      "Epoch [37/40] Batch 1500/2324 Loss D: 0.3497, Loss G: 2.5602\n",
      "Epoch [37/40] Batch 1600/2324 Loss D: 0.3570, Loss G: 2.5331\n",
      "Epoch [37/40] Batch 1700/2324 Loss D: 0.2835, Loss G: 2.6810\n",
      "Epoch [37/40] Batch 1800/2324 Loss D: 0.3374, Loss G: 2.4613\n",
      "Epoch [37/40] Batch 1900/2324 Loss D: 0.4451, Loss G: 2.4158\n",
      "Epoch [37/40] Batch 2000/2324 Loss D: 0.4601, Loss G: 2.3024\n",
      "Epoch [37/40] Batch 2100/2324 Loss D: 0.3988, Loss G: 2.4715\n",
      "Epoch [37/40] Batch 2200/2324 Loss D: 0.3865, Loss G: 2.2345\n",
      "Epoch [37/40] Batch 2300/2324 Loss D: 0.3546, Loss G: 2.3644\n",
      "Saved generator state dict to generator.pth after epoch 38\n",
      "Epoch [38/40] Batch 0/2324 Loss D: 0.3272, Loss G: 2.2632\n",
      "Epoch [38/40] Batch 100/2324 Loss D: 0.3700, Loss G: 2.7409\n",
      "Epoch [38/40] Batch 200/2324 Loss D: 0.3973, Loss G: 2.1847\n",
      "Epoch [38/40] Batch 300/2324 Loss D: 0.3425, Loss G: 2.5570\n",
      "Epoch [38/40] Batch 400/2324 Loss D: 0.4008, Loss G: 2.4370\n",
      "Epoch [38/40] Batch 500/2324 Loss D: 0.4220, Loss G: 2.0793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/40] Batch 600/2324 Loss D: 0.4190, Loss G: 2.4725\n",
      "Epoch [38/40] Batch 700/2324 Loss D: 0.2748, Loss G: 2.8269\n",
      "Epoch [38/40] Batch 800/2324 Loss D: 0.3380, Loss G: 2.7182\n",
      "Epoch [38/40] Batch 900/2324 Loss D: 0.3328, Loss G: 2.9193\n",
      "Epoch [38/40] Batch 1000/2324 Loss D: 0.4037, Loss G: 2.7439\n",
      "Epoch [38/40] Batch 1100/2324 Loss D: 0.4748, Loss G: 2.2816\n",
      "Epoch [38/40] Batch 1200/2324 Loss D: 0.2668, Loss G: 2.4065\n",
      "Epoch [38/40] Batch 1300/2324 Loss D: 0.3848, Loss G: 2.4450\n",
      "Epoch [38/40] Batch 1400/2324 Loss D: 0.4215, Loss G: 2.3875\n",
      "Epoch [38/40] Batch 1500/2324 Loss D: 0.3418, Loss G: 2.4688\n",
      "Epoch [38/40] Batch 1600/2324 Loss D: 0.3850, Loss G: 2.5175\n",
      "Epoch [38/40] Batch 1700/2324 Loss D: 0.4640, Loss G: 2.2397\n",
      "Epoch [38/40] Batch 1800/2324 Loss D: 0.3814, Loss G: 2.4905\n",
      "Epoch [38/40] Batch 1900/2324 Loss D: 0.4044, Loss G: 2.4253\n",
      "Epoch [38/40] Batch 2000/2324 Loss D: 0.3720, Loss G: 2.0917\n",
      "Epoch [38/40] Batch 2100/2324 Loss D: 0.3915, Loss G: 2.3904\n",
      "Epoch [38/40] Batch 2200/2324 Loss D: 0.4542, Loss G: 2.4639\n",
      "Epoch [38/40] Batch 2300/2324 Loss D: 0.3471, Loss G: 2.3231\n",
      "Saved generator state dict to generator.pth after epoch 39\n",
      "Epoch [39/40] Batch 0/2324 Loss D: 0.3865, Loss G: 2.3397\n",
      "Epoch [39/40] Batch 100/2324 Loss D: 0.3776, Loss G: 2.0337\n",
      "Epoch [39/40] Batch 200/2324 Loss D: 0.3977, Loss G: 2.3421\n",
      "Epoch [39/40] Batch 300/2324 Loss D: 0.3705, Loss G: 2.2526\n",
      "Epoch [39/40] Batch 400/2324 Loss D: 0.3080, Loss G: 2.4921\n",
      "Epoch [39/40] Batch 500/2324 Loss D: 0.2816, Loss G: 2.5302\n",
      "Epoch [39/40] Batch 600/2324 Loss D: 0.4264, Loss G: 2.2413\n",
      "Epoch [39/40] Batch 700/2324 Loss D: 0.4647, Loss G: 2.3153\n",
      "Epoch [39/40] Batch 800/2324 Loss D: 0.3250, Loss G: 2.2814\n",
      "Epoch [39/40] Batch 900/2324 Loss D: 0.3926, Loss G: 2.4485\n",
      "Epoch [39/40] Batch 1000/2324 Loss D: 0.3838, Loss G: 2.2186\n",
      "Epoch [39/40] Batch 1100/2324 Loss D: 0.3491, Loss G: 2.3419\n",
      "Epoch [39/40] Batch 1200/2324 Loss D: 0.3687, Loss G: 2.0535\n",
      "Epoch [39/40] Batch 1300/2324 Loss D: 0.5363, Loss G: 2.1620\n",
      "Epoch [39/40] Batch 1400/2324 Loss D: 0.2854, Loss G: 2.3388\n",
      "Epoch [39/40] Batch 1500/2324 Loss D: 0.4490, Loss G: 1.9999\n",
      "Epoch [39/40] Batch 1600/2324 Loss D: 0.3095, Loss G: 2.6379\n",
      "Epoch [39/40] Batch 1700/2324 Loss D: 0.3871, Loss G: 2.2716\n",
      "Epoch [39/40] Batch 1800/2324 Loss D: 0.3587, Loss G: 2.2682\n",
      "Epoch [39/40] Batch 1900/2324 Loss D: 0.3580, Loss G: 2.0442\n",
      "Epoch [39/40] Batch 2000/2324 Loss D: 0.3924, Loss G: 2.0676\n",
      "Epoch [39/40] Batch 2100/2324 Loss D: 0.2826, Loss G: 2.2248\n",
      "Epoch [39/40] Batch 2200/2324 Loss D: 0.3861, Loss G: 2.3070\n",
      "Epoch [39/40] Batch 2300/2324 Loss D: 0.3179, Loss G: 2.2351\n",
      "Saved generator state dict to generator.pth after epoch 40\n",
      "Training completed in: 44.18 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (real, labels, styles) in enumerate(train_loader):\n",
    "        real = real.to(device)\n",
    "        labels = labels.to(device)\n",
    "        styles = styles.to(device)\n",
    "        batch_size = real.size(0)\n",
    "\n",
    "        # Generate fake images\n",
    "        noise = torch.randn(batch_size, latent_dim).to(device)\n",
    "        fake = generator(noise, labels, styles)\n",
    "\n",
    "        # Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "        disc_real = discriminator(real, labels, styles).view(-1)\n",
    "        lossD_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        disc_fake = discriminator(fake.detach(), labels, styles).view(-1)\n",
    "        lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        lossD = (lossD_real + lossD_fake) / 2\n",
    "        discriminator.zero_grad()\n",
    "        lossD.backward()\n",
    "        opt_disc.step()\n",
    "\n",
    "        # Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z)))\n",
    "        output = discriminator(fake, labels, styles).view(-1)\n",
    "        lossG = criterion(output, torch.ones_like(output))\n",
    "        generator.zero_grad()\n",
    "        lossG.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        # Print progress\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{epochs}] Batch {batch_idx}/{len(train_loader)} Loss D: {lossD:.4f}, Loss G: {lossG:.4f}\")\n",
    "\n",
    "    torch.save(generator.state_dict(), save_generator)\n",
    "    print(f\"Saved generator state dict to {save_generator} after epoch {epoch + 1}\")\n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "training_duration = end_time - start_time\n",
    "training_duration_minutes = training_duration / 60\n",
    "print(f\"Training completed in: {training_duration_minutes:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ecef67d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = Generator(latent_dim, label_dim, style_dim, img_dim).to(device)\n",
    "\n",
    "generator.load_state_dict(torch.load(save_generator, map_location=device))\n",
    "generator.eval()\n",
    "\n",
    "class_label = 1\n",
    "style_label = 1\n",
    "\n",
    "# Create one-hot encoded labels\n",
    "class_one_hot = torch.eye(num_classes)[class_label].unsqueeze(0).to(device)\n",
    "style_one_hot = torch.eye(num_styles)[style_label].unsqueeze(0).to(device)\n",
    "\n",
    "num_images = 5\n",
    "\n",
    "noise = torch.randn(num_images, latent_dim).to(device)\n",
    "\n",
    "labels = class_one_hot.repeat(num_images, 1)\n",
    "styles = style_one_hot.repeat(num_images, 1)\n",
    "\n",
    "# Generate fake images using the generator\n",
    "with torch.no_grad():\n",
    "    fake_images = generator(noise, labels, styles)\n",
    "\n",
    "# Reshape the generated images for visualization\n",
    "fake_images = fake_images.view(-1, 1, 28, 28)\n",
    "\n",
    "for i in range(num_images):\n",
    "    plt.figure()\n",
    "    plt.imshow(fake_images[i].cpu().numpy().squeeze(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5db44f7",
   "metadata": {},
   "source": [
    "# Quantitative Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f912473f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MSE for C-VAE: 0.0670\n",
      "Average SSIM for C-VAE: 0.3881\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "mse_scores = []\n",
    "ssim_scores = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, label, style in test_loader:\n",
    "        recon_batch, mu, logvar = cvae(data, label, style)\n",
    "        \n",
    "        mse = mean_squared_error(data.view(-1, 784).cpu().numpy(), recon_batch.cpu().numpy())\n",
    "        mse_scores.append(mse)\n",
    "        \n",
    "        for i in range(len(data)):\n",
    "            original_img = data[i].view(28, 28).cpu().numpy()\n",
    "            reconstructed_img = recon_batch[i].view(28, 28).cpu().numpy()\n",
    "            ssim_score = ssim(original_img, reconstructed_img, data_range=reconstructed_img.max() - reconstructed_img.min())\n",
    "            ssim_scores.append(ssim_score)\n",
    "\n",
    "print(f\"Average MSE for C-VAE: {np.mean(mse_scores):.4f}\")\n",
    "print(f\"Average SSIM for C-VAE: {np.mean(ssim_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c3ddb947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for C-GAN: 0.2839\n"
     ]
    }
   ],
   "source": [
    "def generate_cgan_images(generator, class_label, style_label, num_images=5):\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn(num_images, latent_dim).to(device)\n",
    "        class_one_hot = torch.eye(num_classes)[torch.tensor([class_label] * num_images)].to(device)\n",
    "        style_one_hot = torch.eye(num_styles)[torch.tensor([style_label] * num_images)].to(device)\n",
    "        generated_images = generator(noise, class_one_hot, style_one_hot)\n",
    "        return generated_images.view(-1, 1, 28, 28)\n",
    "\n",
    "fake_images = generate_cgan_images(generator, class_label=1, style_label=1, num_images=100)\n",
    "\n",
    "real_images_flattened = test_images[:100].view(-1, 784).cpu().numpy()\n",
    "fake_images_flattened = fake_images.view(-1, 784).cpu().numpy()\n",
    "\n",
    "mse_cgan = mean_squared_error(real_images_flattened, fake_images_flattened)\n",
    "print(f\"MSE for C-GAN: {mse_cgan:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af589ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
